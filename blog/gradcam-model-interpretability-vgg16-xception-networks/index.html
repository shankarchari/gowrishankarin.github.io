<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=preload as=font href=https://www.gowrishankar.info/fonts/vendor/jost/jost-v4-latin-regular.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=https://www.gowrishankar.info/fonts/vendor/jost/jost-v4-latin-700.woff2 type=font/woff2 crossorigin><link rel=stylesheet href=https://www.gowrishankar.info/main.cbe6761f5adb9ec38962b52f59fe61b74d7af20296267ab35a185e125109f79434987b0deb1827deedf23d9bcb7c554c77fa6052c01fa98769c7bb1939e3193c.css integrity="sha512-y+Z2H1rbnsOJYrUvWf5ht0168gKWJnqzWhheElEJ95Q0mHsN6xgn3u3yPZvLfFVMd/pgUsAfqYdpx7sZOeMZPA==" crossorigin=anonymous><noscript><style>img.lazyload{display:none}</style></noscript><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><title>GradCAM, Model Interpretability - VGG16 & Xception Networks - Gowri Shankar</title><meta name=description content="The objective of this post is to understand the importance of Visual Explanations for CNN based large scale Deep Neural Network Models."><link rel=canonical href=https://www.gowrishankar.info/blog/gradcam-model-interpretability-vgg16-xception-networks/><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.gowrishankar.info/profile.png"><meta name=twitter:title content="GradCAM, Model Interpretability - VGG16 & Xception Networks"><meta name=twitter:description content="The objective of this post is to understand the importance of Visual Explanations for CNN based large scale Deep Neural Network Models."><meta name=twitter:site content="@gowrishankarin"><meta name=twitter:creator content="@gowrishankarin"><meta property="og:title" content="GradCAM, Model Interpretability - VGG16 & Xception Networks"><meta property="og:description" content="The objective of this post is to understand the importance of Visual Explanations for CNN based large scale Deep Neural Network Models."><meta property="og:type" content="article"><meta property="og:url" content="https://www.gowrishankar.info/blog/gradcam-model-interpretability-vgg16-xception-networks/"><meta property="og:image" content="https://www.gowrishankar.info/profile.png"><meta property="article:published_time" content="2020-07-04T09:19:42+01:00"><meta property="article:modified_time" content="2021-02-28T09:19:42+01:00"><meta property="og:site_name" content="Gowri Shankar"><meta property="article:publisher" content="https://www.facebook.com/verlinde.henk"><meta property="article:author" content="https://www.facebook.com/verlinde.henk"><meta property="og:locale" content="en_US"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/www.gowrishankar.info\/blog\/gradcam-model-interpretability-vgg16-xception-networks\/"},"headline":"GradCAM, Model Interpretability - VGG16 \u0026 Xception Networks","image":[],"datePublished":"2020-07-04T09:19:42CET","dateModified":"2021-02-28T09:19:42CET","author":{"@type":"Organization","name":"Gowri Shankar"},"publisher":{"@type":"Organization","name":"Gowri Shankar","logo":{"@type":"ImageObject","url":"https:\/\/www.gowrishankar.info\/logo-doks.png"}},"description":"The objective of this post is to understand the importance of Visual Explanations for CNN based large scale Deep Neural Network Models."}</script><script type=application/ld+json>{"@context":"http://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"https:\/\/www.gowrishankar.info\/"},{"@type":"ListItem","position":2,"name":"Blog","item":"https:\/\/www.gowrishankar.info\/blog\/"},{"@type":"ListItem","position":3,"name":"Gradcam Model Interpretability Vgg16 Xception Networks","item":"https:\/\/www.gowrishankar.info\/blog\/gradcam-model-interpretability-vgg16-xception-networks\/"}]}</script><meta name=theme-color content="#fff"><link rel=apple-touch-icon sizes=180x180 href=https://www.gowrishankar.info/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://www.gowrishankar.info/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://www.gowrishankar.info/favicon-16x16.png><link rel=manifest href=https://www.gowrishankar.info/site.webmanifest><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-K84DCXJ');</script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
</script></head><body class="blog single"><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-K84DCXJ" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><div class="header-bar fixed-top"></div><header class="navbar fixed-top navbar-expand-md navbar-light"><div class=container><input class="menu-btn order-0" type=checkbox id=menu-btn>
<label class="menu-icon d-md-none" for=menu-btn><span class=navicon></span></label><a class="navbar-brand order-1 order-md-0 mr-auto" href=https://www.gowrishankar.info/>Gowri Shankar</a>
<button id=mode class="btn btn-link order-2 order-md-4" type=button aria-label="Toggle mode">
<span class=toggle-dark><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></span><span class=toggle-light><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></span></button><ul class="navbar-nav social-nav order-3 order-md-5"><li class=nav-item><a class=nav-link href=https://github.com/gowrishankarin><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77a5.44 5.44.0 00-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg><span class="ml-2 sr-only">GitHub</span></a></li></ul><div class="collapse navbar-collapse order-4 order-md-1"><ul class="navbar-nav main-nav mr-auto order-5 order-md-2"><li class=nav-item><a class=nav-link href=https://www.gowrishankar.info/docs/prologue/education/>Accolades</a></li><li class="nav-item active"><a class=nav-link href=https://www.gowrishankar.info/blog/>Blog</a></li><li class=nav-item><a class=nav-link href=https://www.gowrishankar.info/reads/>Bookshelf</a></li><li class=nav-item><a class=nav-link href=https://www.gowrishankar.info/contributors/gowri-shankar>Contact</a></li></ul><div class="break order-6 d-md-none"></div><form class="navbar-form flex-grow-1 order-7 order-md-3"><input id=userinput class="form-control is-search" type=search placeholder="Search docs..." aria-label="Search docs..." autocomplete=off><div id=suggestions class="shadow bg-white rounded"></div></form></div></div></header><div class="wrap container" role=document><div class=content><div class="row justify-content-left"><div class=col-12><article><div class=blog-header><h1>GradCAM, Model Interpretability - VGG16 & Xception Networks</h1><p><small>Posted July 4, 2020 by <a class="stretched-link position-relative" href=https://www.gowrishankar.info/contributors/gowri-shankar/>Gowri Shankar</a>&nbsp;&dash;&nbsp;<strong>11&nbsp;min read</strong></small><p></div><p class=lead>The objective of this post is to understand the importance of Visual Explanations for CNN based large scale Deep Neural Network Models.</p><h3 id=objective>Objective</h3><p>The objective of this post is to understand the importance of &ldquo;Visual Explanations&rdquo; for CNN based large scale Deep Neural Network Models.<br>I will be proud if one feels, this post is nothing but a commentary on <strong>R.R Selvaraju et al</strong> paper in <strong>IEEE ICCV, 2017</strong> title <a href=https://ieeexplore.ieee.org/document/8237336>Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization</a></p><h3 id=goal>Goal</h3><p>Decipher these Formulas and Program them using Keras and Numpy
$$y_c = \frac{\partial{y^c}}{\partial{A^k}}$$</p><p>$$Weights_{NeuronImportance} = \alpha^c_k = \frac{1}{Z} \sum_{i=1}^u \sum_{i=1}^{v} y_c$$
$$i.e.$$
$$\alpha^c_k = \frac{1}{Z} \sum_{i=1}^u \sum_{i=1}^{v} \frac{\partial{y^c}}{\partial{A^k}}$$</p><p><strong>Top 6 Predictions and GradCAM Heatmaps of VGG16 for a Sample Image</strong></p><p><img src=https://raw.githubusercontent.com/gowrishankarin/data_science/master/topics/dl/model_explanation/anime.gif alt=Heatmap></p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=c1># This Python 3 environment comes with many helpful analytics libraries installed</span>
<span class=c1># It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python</span>
<span class=c1># For example, here&#39;s several helpful packages to load</span>

<span class=kn>import</span> <span class=nn>numpy</span> <span class=kn>as</span> <span class=nn>np</span> <span class=c1># linear algebra</span>
<span class=kn>import</span> <span class=nn>pandas</span> <span class=kn>as</span> <span class=nn>pd</span> <span class=c1># data processing, CSV file I/O (e.g. pd.read_csv)</span>

<span class=c1># Input data files are available in the read-only &#34;../input/&#34; directory</span>
<span class=c1># For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory</span>

<span class=kn>import</span> <span class=nn>os</span>
<span class=k>for</span> <span class=n>dirname</span><span class=p>,</span> <span class=n>_</span><span class=p>,</span> <span class=n>filenames</span> <span class=ow>in</span> <span class=n>os</span><span class=o>.</span><span class=n>walk</span><span class=p>(</span><span class=s1>&#39;/kaggle/input/sample-images&#39;</span><span class=p>):</span>
    <span class=k>for</span> <span class=n>filename</span> <span class=ow>in</span> <span class=n>filenames</span><span class=p>:</span>
        <span class=k>print</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>dirname</span><span class=p>,</span> <span class=n>filename</span><span class=p>))</span>

<span class=c1># You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &#34;Save &amp; Run All&#34; </span>
<span class=c1># You can also write temporary files to /kaggle/temp/, but they won&#39;t be saved outside of the current session</span>
</code></pre></div><pre><code>/kaggle/input/sample-images/xception.png
/kaggle/input/sample-images/vgg16_dock.png
/kaggle/input/sample-images/motor_boat.jpg
/kaggle/input/sample-images/dock.jpg
/kaggle/input/sample-images/vgg16_pier.png
/kaggle/input/sample-images/newyork.jpg
/kaggle/input/sample-images/newyork_2.jpg
/kaggle/input/sample-images/anime.gif
/kaggle/input/sample-images/boat_2.jpg
/kaggle/input/sample-images/pier.jpg
/kaggle/input/sample-images/imagenet_1000_idx.js
</code></pre><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>tensorflow</span> <span class=kn>as</span> <span class=nn>tf</span>
<span class=kn>from</span> <span class=nn>tensorflow</span> <span class=kn>import</span> <span class=n>keras</span>
<span class=kn>import</span> <span class=nn>cv2</span>
<span class=kn>import</span> <span class=nn>json</span>
<span class=kn>import</span> <span class=nn>imageio</span>
<span class=kn>import</span> <span class=nn>glob</span>

<span class=kn>from</span> <span class=nn>IPython.display</span> <span class=kn>import</span> <span class=n>Image</span>
<span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=kn>as</span> <span class=nn>plt</span>
<span class=kn>import</span> <span class=nn>matplotlib.cm</span> <span class=kn>as</span> <span class=nn>cm</span>

<span class=n>FILE_PATH</span> <span class=o>=</span> <span class=s2>&#34;/kaggle/input/sample-images/&#34;</span>
</code></pre></div><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>display</span><span class=p>(</span><span class=n>Image</span><span class=p>(</span><span class=n>FILE_PATH</span><span class=o>+</span><span class=s2>&#34;vgg16_dock.png&#34;</span><span class=p>))</span>
</code></pre></div><p><img src=output_6_0.png alt=png></p><h3 id=is-this-topic-relevant-to-siim-isic-melanoma-detection>Is this topic relevant to SIIM-ISIC Melanoma Detection?</h3><p>Yes, Its is. If you feel the same, please make me aware - We are in same page.</p><h3 id=notebook-organization>Notebook Organization</h3><p><strong>PART 1: GradCAM Theory</strong></p><ul><li>Problem Statement</li><li>Seeing is Believing</li><li>Grad-CAM Rationale</li><li>The Approach</li><li>The Algorithm - Math Involved<ul><li>Gradient calculation via Backpropagation</li><li>Global Average Pooled Gradients</li><li>ReLU activation on Weights</li></ul></li></ul><p><strong>PART 2: Demo using Keras & Tensorflow</strong><br><strong>Section 1</strong>.</p><ul><li>VGG16 and Xception Network Properties</li><li>Image Preprocessing</li><li>Step by Step Implementation of Heatmap Calculation</li><li>Superimposition of Heatmap on Original Image</li></ul><p><strong>Section 2</strong>.</p><ul><li>Exploration of Top - 9 Predictions and their Heatmap</li></ul><p><strong>Section 3</strong>.</p><ul><li>Compute GradCAM heatmaps for predefined classes on an image.</li></ul><h1 id=gradcam-theory>GradCAM Theory</h1><h2 id=the-problem>The Problem.</h2><p>Significant breakthroughs are achieved in computer vision during the past decade or so using Deep Neural Networks based on Convolutional Neural Network architecture. Following are some of the key areas where CNN based models drew extraordinary results,</p><ol><li>Image Classification</li><li>Object Detection</li><li>Semantic Segmentation</li><li>Image Captioning</li><li>Visual Question Answering</li><li>Visual Dialog and</li><li>Embodied Question Answering</li></ol><p>Though the performance of them are great and intuitive, they fail when it comes to model explanation and interpretability.</p><ul><li>There is always a trade-off between accuracy and interpretability</li><li>Lack of decomposability result in lack of explainable/interpretable models</li><li>High complexity make interpretability hard. For e.g 200+ layer depth models like ResNets</li><li>Explainable models are usually lack accuracy scores</li></ul><p>However by making a model explainable and interpretable trust and faith on the model naturally increases exponentially.</p><h3 id=gradcam-architecture>GradCAM Architecture</h3><p>Image Ref: <a href=http://gradcam.cloudcv.org/>http://gradcam.cloudcv.org/</a></p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>display</span><span class=p>(</span><span class=n>Image</span><span class=p>(</span><span class=s1>&#39;http://gradcam.cloudcv.org/static/images/network.png&#39;</span><span class=p>))</span>
</code></pre></div><p><img src=output_10_0.png alt=png></p><h2 id=seeing-is-believing>Seeing is Believing.</h2><p>Building a model that have the ability to explain why they predict what they predict results in transparency and trustworthiness. This is achieved by</p><ul><li>Highlighting important pixels of predictios</li><li>Visualizing partial derivatives of predicted scores wrt pixel intensities</li><li>Deconvolution by making modifications to raw gradients</li><li>Methods that sythesize images to maximally activate a network unit</li><li>Invert a latent representation</li></ul><p>The above all has good visual explainability. However they are not class discriminative. They tend to result in visualizations wrt different classes are nearly identical and lack finesse</p><p>Then what are the criteria for a good visual explanation.</p><ul><li>Class Discriminative - i.e. localize the category in the image</li><li>High Resolution - i.e. Capture fine-grained detail</li></ul><h2 id=why-grad-cam>Why Grad-CAM?</h2><p>Gradient-weighted Class Activation Mapping(GradCAM) techniques comes with comprehensive answers for all above challenges.</p><ul><li>They are class-discriminative using localization techniques</li><li>They generate comprehendable visual explantaion for most of the CNN models</li><li>They work without changing the architecture of the model</li><li>They do not seek retraining the model</li><li>They can be applied to widely acclaimed models lik ResNets, Xception, Inception etc with minimum effort</li><li>They perform well with CNN+LSTM or Text Explanationable networks</li></ul><h2 id=the-approach>The Approach</h2><ul><li>Single shot localization through single forward and a partial backward pass per image</li><li>Convolutional layers naturally retail spatial information that is lost in fully connected layers</li><li>Last Conv layers to have the best compromize between high level semantics and detailed spatial info</li><li>Neurons here look for semantic class specific info in the image</li><li>GradCAM uses gradient information flow into the last conv layer to assign importance values to each neurons for a class of interest</li><li>This can be applied for any layer of a DNN</li></ul><h2 id=the-algorithm>The Algorithm.</h2><p>The neurons in the last convolution layers concentrate on semantic class specific info of the input image. That is object/class specific parts of the whole image through segmentation. GradCAM fetches the gradient information flowing and assign importance values to each neurons.</p><h3 id=gradient-via-backpropagation>Gradient via Backpropagation</h3><p>Let us call,<br>The class discriminative localization map is $L_{GradCAM}^c \in \ R ^{u \times v}$</p><p>Where,<br>$L$ is the GradCAM for a particular class $c$<br>$u$ and $v$ are the width and height of the class $c$</p><p>Further,<br>$y_c$ is the gradient score for the class $c$ before softmax classification<br>Gradient score is wrt $A^k$ the feature map activations of the last convolution layer. ie</p><p>$$y_c = \frac{\partial{y^c}}{\partial{A^k}}\tag{1. Gradients via Backpropagation}$$</p><h3 id=global-average-pooling-on-activation-maps>Global Average Pooling on Activation Maps</h3><p>Usually the gradients $y_c$ flown back are Globally Average pooled over width and height but vary wrt architecture. Ref profperties of VGG16 and Xception.</p><p>$$Weights_{NeuronImportance} = \alpha^c_k = \frac{1}{Z} \sum_{i=1}^u \sum_{i=1}^{v} y_c \tag{Global Average Pooled Gradients}$$
$$i.e.$$
$$\alpha^c_k = \frac{1}{Z} \sum_{i=1}^u \sum_{i=1}^{v} \frac{\partial{y^c}}{\partial{A^k}} \tag{2}$$</p><h3 id=gradcam-activation-maps>GradCAM Activation Maps</h3><ul><li>The weights calculated represents the partial linearization of the DNN downstream</li><li>It captures the importance feature map $k$ for the target class $c$</li></ul><p>Perform ReLU on the weighted combination of forward activation maps</p><p>$$L_{GradCAM}^c = ReLU(,\sum_{i=1}^{k}\alpha^c_k) ,$$</p><ul><li>This operation results in coarse heatmap of the same size as the convolution feature maps (VGG16 $(14 \times 14)$, Xception $(10 \times 10)$)</li><li>a ReLU is applied because, only the positive influence on the class of interest is considered</li><li>i.e Negative influence are likedly to belong to other categories in the image</li><li>Note: $y_c$ need not be the class score produced by an image classification, it could be any differentiable activation including words from a caption or answer to a question, in machine translation problems.</li></ul><h1 id=demo-using-keras--tensorflow>Demo using Keras & Tensorflow</h1><h3 id=vgg16-and-xception-properties>VGG16 and Xception Properties</h3><p>We shall demonstrate GradCAM approach on 2 widely accepted CNN Networks VGG16 and Xception. Following are the properties and one could extend this to other networks&mldr;</p><p><strong>VGG16</strong></p><ul><li>Input Image Size is (224, 224)</li><li>Last Convolution Layer Name: block5_conv3</li><li>Last Classifier Layers after Conv Layers: 5</li><li>Heatmap Dimension: (14, 14)</li></ul><p>Ref Image: <a href=https://www.researchgate.net/figure/Fig-A1-The-standard-VGG-16-network-architecture-as-proposed-in-32-Note-that-only_fig3_322512435>Research Gate, Max Fergusan</a></p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>display</span><span class=p>(</span><span class=n>Image</span><span class=p>(</span><span class=s2>&#34;https://www.researchgate.net/profile/Max_Ferguson/publication/322512435/figure/fig3/AS:697390994567179@1543282378794/Fig-A1-The-standard-VGG-16-network-architecture-as-proposed-in-32-Note-that-only.png&#34;</span><span class=p>))</span>
</code></pre></div><p><img src=output_19_0.png alt=png></p><p><strong>Xception</strong></p><ul><li>Input Image Size is (299, 299)</li><li>Last Convolution Layer Name: block14_sepconv2_act</li><li>Last Classifier Layers after Conv Layers: 2</li><li>Heatmap Dimension: (10, 10)</li></ul><p>Ref Image: <a href=https://towardsdatascience.com/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568>Review of Xception by Sik-Ho Tsang</a></p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>display</span><span class=p>(</span><span class=n>Image</span><span class=p>(</span><span class=n>FILE_PATH</span><span class=o>+</span><span class=s2>&#34;xception.png&#34;</span><span class=p>))</span>
</code></pre></div><p><img src=output_21_0.png alt=png></p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>properties</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s2>&#34;vgg16&#34;</span><span class=p>:</span> <span class=p>{</span>
        <span class=s2>&#34;img_size&#34;</span><span class=p>:</span> <span class=p>(</span><span class=mi>224</span><span class=p>,</span> <span class=mi>224</span><span class=p>),</span>
        <span class=s2>&#34;last_conv_layer&#34;</span><span class=p>:</span> <span class=s2>&#34;block5_conv3&#34;</span><span class=p>,</span>
        <span class=s2>&#34;last_classifier_layers&#34;</span><span class=p>:</span> <span class=p>[</span>
            <span class=s2>&#34;block5_pool&#34;</span><span class=p>,</span>
            <span class=s2>&#34;flatten&#34;</span><span class=p>,</span>
            <span class=s2>&#34;fc1&#34;</span><span class=p>,</span>
            <span class=s2>&#34;fc2&#34;</span><span class=p>,</span>
            <span class=s2>&#34;predictions&#34;</span><span class=p>,</span>
        <span class=p>],</span>
        <span class=s2>&#34;model_builder&#34;</span><span class=p>:</span> <span class=n>keras</span><span class=o>.</span><span class=n>applications</span><span class=o>.</span><span class=n>vgg16</span><span class=o>.</span><span class=n>VGG16</span><span class=p>,</span>
        <span class=s2>&#34;preprocess_input&#34;</span><span class=p>:</span> <span class=n>keras</span><span class=o>.</span><span class=n>applications</span><span class=o>.</span><span class=n>vgg16</span><span class=o>.</span><span class=n>preprocess_input</span><span class=p>,</span>
        <span class=s2>&#34;decode_predictions&#34;</span><span class=p>:</span> <span class=n>keras</span><span class=o>.</span><span class=n>applications</span><span class=o>.</span><span class=n>vgg16</span><span class=o>.</span><span class=n>decode_predictions</span><span class=p>,</span>
    <span class=p>},</span>
    <span class=s2>&#34;xception&#34;</span><span class=p>:</span> <span class=p>{</span>
        <span class=s2>&#34;img_size&#34;</span><span class=p>:</span> <span class=p>(</span><span class=mi>299</span><span class=p>,</span> <span class=mi>299</span><span class=p>),</span>
        <span class=s2>&#34;last_conv_layer&#34;</span><span class=p>:</span> <span class=s2>&#34;block14_sepconv2_act&#34;</span><span class=p>,</span>
        <span class=s2>&#34;last_classifier_layers&#34;</span><span class=p>:</span> <span class=p>[</span>
            <span class=s2>&#34;avg_pool&#34;</span><span class=p>,</span>
            <span class=s2>&#34;predictions&#34;</span><span class=p>,</span>
        <span class=p>],</span>
        <span class=s2>&#34;model_builder&#34;</span><span class=p>:</span> <span class=n>keras</span><span class=o>.</span><span class=n>applications</span><span class=o>.</span><span class=n>xception</span><span class=o>.</span><span class=n>Xception</span><span class=p>,</span>
        <span class=s2>&#34;preprocess_input&#34;</span><span class=p>:</span> <span class=n>keras</span><span class=o>.</span><span class=n>applications</span><span class=o>.</span><span class=n>xception</span><span class=o>.</span><span class=n>preprocess_input</span><span class=p>,</span>
        <span class=s2>&#34;decode_predictions&#34;</span><span class=p>:</span> <span class=n>keras</span><span class=o>.</span><span class=n>applications</span><span class=o>.</span><span class=n>xception</span><span class=o>.</span><span class=n>decode_predictions</span><span class=p>,</span>
        
    <span class=p>}</span>
<span class=p>}</span>
</code></pre></div><p>Select the choice of your network and image, here I have taken boat_2.jpg. Also set how many top predictions we want to interospect</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>NETWORK</span> <span class=o>=</span> <span class=s2>&#34;vgg16&#34;</span>
<span class=n>IMG_PATH</span> <span class=o>=</span> <span class=n>FILE_PATH</span> <span class=o>+</span> <span class=s2>&#34;pier.jpg&#34;</span>
<span class=n>IMG_2_PATH</span> <span class=o>=</span> <span class=n>FILE_PATH</span> <span class=o>+</span> <span class=s2>&#34;dock.jpg&#34;</span>
<span class=n>IMG_SIZE</span> <span class=o>=</span> <span class=n>properties</span><span class=p>[</span><span class=n>NETWORK</span><span class=p>][</span><span class=s2>&#34;img_size&#34;</span><span class=p>]</span>
<span class=n>LAST_CONV_LAYER</span> <span class=o>=</span> <span class=n>properties</span><span class=p>[</span><span class=n>NETWORK</span><span class=p>][</span><span class=s2>&#34;last_conv_layer&#34;</span><span class=p>]</span>
<span class=n>CLASSIFIER_LAYER_NAMES</span> <span class=o>=</span> <span class=n>properties</span><span class=p>[</span><span class=n>NETWORK</span><span class=p>][</span><span class=s2>&#34;last_classifier_layers&#34;</span><span class=p>]</span>

<span class=n>TOP_N</span> <span class=o>=</span> <span class=mi>8</span>
</code></pre></div><p>Pick the corresponding model, image preprocessor and prediction decoder</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>model_builder</span> <span class=o>=</span> <span class=n>properties</span><span class=p>[</span><span class=n>NETWORK</span><span class=p>][</span><span class=s2>&#34;model_builder&#34;</span><span class=p>]</span>
<span class=n>preprocess_input</span> <span class=o>=</span> <span class=n>properties</span><span class=p>[</span><span class=n>NETWORK</span><span class=p>][</span><span class=s2>&#34;preprocess_input&#34;</span><span class=p>]</span>
<span class=n>decode_predictions</span> <span class=o>=</span> <span class=n>properties</span><span class=p>[</span><span class=n>NETWORK</span><span class=p>][</span><span class=s2>&#34;decode_predictions&#34;</span><span class=p>]</span>
</code></pre></div><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>display</span><span class=p>(</span><span class=n>Image</span><span class=p>(</span><span class=n>IMG_PATH</span><span class=p>))</span>
</code></pre></div><p><img src=output_27_0.jpg alt=jpeg></p><h2 id=preprocessing>Preprocessing</h2><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>get_img_array</span><span class=p>(</span><span class=n>img_path</span><span class=p>,</span> <span class=n>size</span><span class=p>):</span>
    <span class=n>img</span> <span class=o>=</span> <span class=n>keras</span><span class=o>.</span><span class=n>preprocessing</span><span class=o>.</span><span class=n>image</span><span class=o>.</span><span class=n>load_img</span><span class=p>(</span><span class=n>img_path</span><span class=p>,</span> <span class=n>target_size</span><span class=o>=</span><span class=n>size</span><span class=p>)</span>
    <span class=n>array</span> <span class=o>=</span> <span class=n>keras</span><span class=o>.</span><span class=n>preprocessing</span><span class=o>.</span><span class=n>image</span><span class=o>.</span><span class=n>img_to_array</span><span class=p>(</span><span class=n>img</span><span class=p>)</span>
    <span class=n>array</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>array</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
    <span class=k>return</span> <span class=n>array</span>

<span class=k>def</span> <span class=nf>load_imagenet_classes</span><span class=p>(</span><span class=n>filepath</span><span class=o>=</span><span class=n>FILE_PATH</span> <span class=o>+</span> <span class=s2>&#34;imagenet_1000_idx.js&#34;</span><span class=p>):</span>
    
    <span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>filepath</span><span class=p>,</span> <span class=s1>&#39;r&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=nb>file</span><span class=p>:</span>
        <span class=n>class_dict</span> <span class=o>=</span> <span class=n>json</span><span class=o>.</span><span class=n>loads</span><span class=p>(</span><span class=nb>file</span><span class=o>.</span><span class=n>read</span><span class=p>())</span>
    <span class=n>dict_by_name</span> <span class=o>=</span> <span class=p>{</span><span class=n>class_dict</span><span class=p>[</span><span class=n>key</span><span class=p>]</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s2>&#34;,&#34;</span><span class=p>)[</span><span class=mi>0</span><span class=p>]:</span> <span class=nb>int</span><span class=p>(</span><span class=n>key</span><span class=p>)</span> <span class=k>for</span> <span class=n>key</span> <span class=ow>in</span> <span class=n>class_dict</span><span class=p>}</span>
    <span class=k>return</span> <span class=n>dict_by_name</span><span class=p>,</span> <span class=n>class_dict</span>

<span class=n>DICT_BY_NAME</span><span class=p>,</span> <span class=n>CLASS_DICT</span> <span class=o>=</span> <span class=n>load_imagenet_classes</span><span class=p>()</span>
</code></pre></div><p>Interospect the Predictions of the Input Image</p><ul><li>Preprocess the Image, image to array conversion</li><li>Get the Keras Model</li><li>Predict the Image</li><li>Decode the predictions</li></ul><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>get_predictions</span><span class=p>(</span><span class=n>image_path</span><span class=p>,</span> <span class=n>image_size</span><span class=p>,</span> <span class=n>top_n</span><span class=p>):</span>
    <span class=n>img_array</span> <span class=o>=</span> <span class=n>get_img_array</span><span class=p>(</span><span class=n>image_path</span><span class=p>,</span> <span class=n>size</span><span class=o>=</span><span class=n>image_size</span><span class=p>)</span>
    <span class=n>img_array</span> <span class=o>=</span> <span class=n>preprocess_input</span><span class=p>(</span><span class=n>img_array</span><span class=p>)</span>
    <span class=n>model</span> <span class=o>=</span> <span class=n>model_builder</span><span class=p>(</span><span class=n>weights</span><span class=o>=</span><span class=s2>&#34;imagenet&#34;</span><span class=p>)</span>
    <span class=n>preds</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>img_array</span><span class=p>)</span>
    <span class=n>preds_n</span> <span class=o>=</span> <span class=n>decode_predictions</span><span class=p>(</span><span class=n>preds</span><span class=p>,</span> <span class=n>top</span><span class=o>=</span><span class=n>top_n</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
    <span class=k>return</span> <span class=n>preds_n</span>

<span class=n>preds_n</span> <span class=o>=</span> <span class=n>get_predictions</span><span class=p>(</span><span class=n>IMG_PATH</span><span class=p>,</span> <span class=n>IMG_SIZE</span><span class=p>,</span> <span class=n>TOP_N</span><span class=p>)</span>
</code></pre></div><pre><code>Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5
553467904/553467096 [==============================] - 6s 0us/step
Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json
40960/35363 [==================================] - 0s 0us/step
</code></pre><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>print_predictions</span><span class=p>(</span><span class=n>predictions</span><span class=p>):</span>
    <span class=k>print</span><span class=p>(</span><span class=s2>&#34;Predictions&#34;</span><span class=p>)</span>
    <span class=k>for</span> <span class=n>index</span> <span class=ow>in</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>predictions</span><span class=p>)):</span>
        <span class=k>print</span><span class=p>(</span><span class=n>f</span><span class=s1>&#39;Id: {DICT_BY_NAME[predictions[index][1]]} Probability: {predictions[index][2]:4f} Class Name: {predictions[index][1].capitalize()}&#39;</span><span class=p>)</span>
<span class=n>print_predictions</span><span class=p>(</span><span class=n>preds_n</span><span class=p>)</span>
</code></pre></div><pre><code>Predictions
Id: 718 Probability: 0.770770 Class Name: Pier
Id: 460 Probability: 0.153460 Class Name: Breakwater
Id: 525 Probability: 0.041275 Class Name: Dam
Id: 536 Probability: 0.023470 Class Name: Dock
Id: 694 Probability: 0.002335 Class Name: Paddlewheel
Id: 839 Probability: 0.002191 Class Name: Suspension_bridge
Id: 449 Probability: 0.001863 Class Name: Boathouse
Id: 975 Probability: 0.000897 Class Name: Lakeside
</code></pre><h2 id=make-gradcam-heatmap>Make GradCAM Heatmap</h2><p>Following method works in 2 modes</p><ol><li>Calculate GradCAM heatmaps for top N predictions, provided top_n > 0</li><li>Provide Imagenet index of the classes of interest and get gradient heatmap for the input image.</li></ol><h3 id=steps>STEPS</h3><ol><li>Create a model that maps the input image to the activations of the last convolution layer - Get last conv layer&rsquo;s output dimensions</li><li>Create another model, that maps from last convolution layer to the final class predictions - This is the classifier model that calculated the gradient</li><li>If top N predictions are to be interospected, Get their Imagenet indices else assign the indices given</li><li>Create an array to store the heatmaps</li><li>Iteratively calculate heatmaps for all classes of interest using GradientTape</li><li>Watch the last convolution output during the prediction process to calculate the gradients</li><li>Compute the activations of last conv layer and make the tape to watch</li><li>Get the class predictions and the class channel using the class index</li><li>Using tape, Get the gradient for the predicted class wrt the output feature map of last conv layer</li><li>Calculate the mean intensity of the gradient over its feature map channel</li><li>Multiply each channel in feature map array by weight importance of the channel</li><li>The channel-wise mean of the resulting feature map is our heatmap of class activation</li><li>Normalize the heatmap between [0, 1] for ease of visualization</li></ol><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>get_top_predicted_indices</span><span class=p>(</span><span class=n>predictions</span><span class=p>,</span> <span class=n>top_n</span><span class=p>):</span>
    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>argsort</span><span class=p>(</span><span class=o>-</span><span class=n>predictions</span><span class=p>)</span><span class=o>.</span><span class=n>squeeze</span><span class=p>()[:</span><span class=n>top_n</span><span class=p>]</span>

<span class=k>def</span> <span class=nf>make_gradcam_heatmap</span><span class=p>(</span>
    <span class=n>img_array</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> 
    <span class=n>last_conv_layer_name</span><span class=p>,</span> 
    <span class=n>classifier_layer_names</span><span class=p>,</span>
    <span class=n>top_n</span><span class=p>,</span>
    <span class=n>class_indices</span>
<span class=p>):</span>
    <span class=c1>#1. Create a model that maps the input image to the activations of the last convolution layer - Get last conv layer&#39;s output dimensions</span>
    <span class=n>last_conv_layer</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>get_layer</span><span class=p>(</span><span class=n>last_conv_layer_name</span><span class=p>)</span>
    <span class=n>last_conv_layer_model</span> <span class=o>=</span> <span class=n>keras</span><span class=o>.</span><span class=n>Model</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>inputs</span><span class=p>,</span> <span class=n>last_conv_layer</span><span class=o>.</span><span class=n>output</span><span class=p>)</span>
    
    <span class=c1>#2. Create another model, that maps from last convolution layer to the final class predictions - This is the classifier model that calculated the gradient</span>
    <span class=n>classifier_input</span> <span class=o>=</span> <span class=n>keras</span><span class=o>.</span><span class=n>Input</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=n>last_conv_layer</span><span class=o>.</span><span class=n>output</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>:])</span>
    <span class=n>x</span> <span class=o>=</span> <span class=n>classifier_input</span>
    <span class=k>for</span> <span class=n>layer_name</span> <span class=ow>in</span> <span class=n>classifier_layer_names</span><span class=p>:</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>get_layer</span><span class=p>(</span><span class=n>layer_name</span><span class=p>)(</span><span class=n>x</span><span class=p>)</span>
    <span class=n>classifier_model</span> <span class=o>=</span> <span class=n>keras</span><span class=o>.</span><span class=n>Model</span><span class=p>(</span><span class=n>classifier_input</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>
    
    <span class=c1>#3. If top N predictions are to be interospected, Get their Imagenet indices else assign the indices given</span>
    <span class=k>if</span><span class=p>(</span><span class=n>top_n</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>):</span>
        <span class=n>last_conv_layer_output</span> <span class=o>=</span> <span class=n>last_conv_layer_model</span><span class=p>(</span><span class=n>img_array</span><span class=p>)</span>
        <span class=n>preds</span> <span class=o>=</span> <span class=n>classifier_model</span><span class=p>(</span><span class=n>last_conv_layer_output</span><span class=p>)</span>
        <span class=n>class_indices</span> <span class=o>=</span> <span class=n>get_top_predicted_indices</span><span class=p>(</span><span class=n>preds</span><span class=p>,</span> <span class=n>top_n</span><span class=p>)</span>
    <span class=k>else</span><span class=p>:</span>
        <span class=n>top_n</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>class_indices</span><span class=p>)</span>
    
    <span class=c1>#4. Create an array to store the heatmaps</span>
    <span class=n>heatmaps</span> <span class=o>=</span> <span class=p>[]</span>
    <span class=c1>#5. Iteratively calculate heatmaps for all classes of interest using GradientTape</span>
    <span class=k>for</span> <span class=n>index</span> <span class=ow>in</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>top_n</span><span class=p>):</span>
    
        <span class=c1>#6. Watch the last convolution output during the prediction process to calculate the gradients</span>
        <span class=c1>#7. Compute the activations of last conv layer and make the tape to watch</span>
        <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>GradientTape</span><span class=p>()</span> <span class=k>as</span> <span class=n>tape</span><span class=p>:</span>
            <span class=c1># Compute activations of the last conv layer and make the tape watch it</span>
            <span class=n>last_conv_layer_output</span> <span class=o>=</span> <span class=n>last_conv_layer_model</span><span class=p>(</span><span class=n>img_array</span><span class=p>)</span>
            <span class=n>tape</span><span class=o>.</span><span class=n>watch</span><span class=p>(</span><span class=n>last_conv_layer_output</span><span class=p>)</span>

            <span class=c1>#8. Get the class predictions and the class channel using the class index</span>
            <span class=n>preds</span> <span class=o>=</span> <span class=n>classifier_model</span><span class=p>(</span><span class=n>last_conv_layer_output</span><span class=p>)</span>
            <span class=n>class_channel</span> <span class=o>=</span> <span class=n>preds</span><span class=p>[:,</span> <span class=n>class_indices</span><span class=p>[</span><span class=n>index</span><span class=p>]]</span>
            
        <span class=c1>#9. Using tape, Get the gradient for the predicted class wrt the output feature map of last conv layer    </span>
        <span class=n>grads</span> <span class=o>=</span> <span class=n>tape</span><span class=o>.</span><span class=n>gradient</span><span class=p>(</span>
            <span class=n>class_channel</span><span class=p>,</span>
            <span class=n>last_conv_layer_output</span>
        <span class=p>)</span>
        
        <span class=c1>#10. Calculate the mean intensity of the gradient over its feature map channel</span>
        <span class=n>pooled_grads</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=n>grads</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>    
        <span class=n>last_conv_layer_output</span> <span class=o>=</span> <span class=n>last_conv_layer_output</span><span class=o>.</span><span class=n>numpy</span><span class=p>()[</span><span class=mi>0</span><span class=p>]</span>
        <span class=n>pooled_grads</span> <span class=o>=</span> <span class=n>pooled_grads</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
        
        <span class=c1>#11. Multiply each channel in feature map array by weight importance of the channel</span>
        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>pooled_grads</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]):</span>
            <span class=n>last_conv_layer_output</span><span class=p>[:,</span> <span class=p>:,</span> <span class=n>i</span><span class=p>]</span> <span class=o>*=</span> <span class=n>pooled_grads</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>

        <span class=c1>#12. The channel-wise mean of the resulting feature map is our heatmap of class activation</span>
        <span class=n>heatmap</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>last_conv_layer_output</span><span class=p>,</span> <span class=n>axis</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>

        <span class=c1>#13. Normalize the heatmap between [0, 1] for ease of visualization</span>
        <span class=n>heatmap</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>maximum</span><span class=p>(</span><span class=n>heatmap</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>heatmap</span><span class=p>)</span>

        <span class=n>heatmaps</span><span class=o>.</span><span class=n>append</span><span class=p>({</span>
            <span class=s2>&#34;class_id&#34;</span><span class=p>:</span> <span class=n>class_indices</span><span class=p>[</span><span class=n>index</span><span class=p>],</span>
            <span class=s2>&#34;heatmap&#34;</span><span class=p>:</span> <span class=n>heatmap</span>
        <span class=p>})</span>

    <span class=k>return</span> <span class=n>heatmaps</span>
</code></pre></div><h2 id=exploration-of-top-n-predictions>Exploration of Top N predictions</h2><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=c1># Calculate Heatmaps for TOP_N Predictions</span>
<span class=n>heatmaps</span> <span class=o>=</span> <span class=n>make_gradcam_heatmap</span><span class=p>(</span>
    <span class=n>get_img_array</span><span class=p>(</span><span class=n>IMG_PATH</span><span class=p>,</span> <span class=n>IMG_SIZE</span><span class=p>),</span> 
    <span class=n>model_builder</span><span class=p>(</span><span class=n>weights</span><span class=o>=</span><span class=s2>&#34;imagenet&#34;</span><span class=p>),</span> 
    <span class=n>LAST_CONV_LAYER</span><span class=p>,</span> 
    <span class=n>CLASSIFIER_LAYER_NAMES</span><span class=p>,</span> 
    <span class=n>TOP_N</span><span class=p>,</span> 
    <span class=bp>None</span>
<span class=p>)</span>
</code></pre></div><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>superimpose_heatmap</span><span class=p>(</span><span class=n>image_path</span><span class=p>,</span> <span class=n>heatmap</span><span class=p>):</span>
    <span class=n>img</span> <span class=o>=</span> <span class=n>keras</span><span class=o>.</span><span class=n>preprocessing</span><span class=o>.</span><span class=n>image</span><span class=o>.</span><span class=n>load_img</span><span class=p>(</span><span class=n>image_path</span><span class=p>)</span>
    <span class=n>img</span> <span class=o>=</span> <span class=n>keras</span><span class=o>.</span><span class=n>preprocessing</span><span class=o>.</span><span class=n>image</span><span class=o>.</span><span class=n>img_to_array</span><span class=p>(</span><span class=n>img</span><span class=p>)</span>
    
    <span class=c1># We rescale heatmap to a range 0-255</span>
    <span class=n>heatmap</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>uint8</span><span class=p>(</span><span class=mi>255</span> <span class=o>*</span> <span class=n>heatmap</span><span class=p>)</span>
    <span class=n>heatmap</span> <span class=o>=</span> <span class=n>cv2</span><span class=o>.</span><span class=n>applyColorMap</span><span class=p>(</span><span class=n>heatmap</span><span class=p>,</span> <span class=n>cv2</span><span class=o>.</span><span class=n>COLORMAP_JET</span><span class=p>)</span>
    <span class=n>heatmap</span> <span class=o>=</span> <span class=n>keras</span><span class=o>.</span><span class=n>preprocessing</span><span class=o>.</span><span class=n>image</span><span class=o>.</span><span class=n>array_to_img</span><span class=p>(</span><span class=n>heatmap</span><span class=p>)</span>
    <span class=n>heatmap</span> <span class=o>=</span> <span class=n>heatmap</span><span class=o>.</span><span class=n>resize</span><span class=p>((</span><span class=n>img</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>img</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]))</span>
    
    <span class=n>heatmap</span> <span class=o>=</span> <span class=n>keras</span><span class=o>.</span><span class=n>preprocessing</span><span class=o>.</span><span class=n>image</span><span class=o>.</span><span class=n>img_to_array</span><span class=p>(</span><span class=n>heatmap</span><span class=p>)</span>
    <span class=n>superimposed_img</span> <span class=o>=</span> <span class=n>cv2</span><span class=o>.</span><span class=n>addWeighted</span><span class=p>(</span><span class=n>heatmap</span><span class=p>,</span> <span class=mf>0.4</span><span class=p>,</span> <span class=n>img</span><span class=p>,</span> <span class=mf>0.6</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>
    <span class=n>superimposed_img</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>uint8</span><span class=p>(</span><span class=n>superimposed_img</span><span class=p>)</span>
    
    <span class=k>return</span> <span class=n>superimposed_img</span>
</code></pre></div><div class=highlight><pre class=chroma><code class=language-python data-lang=python>
<span class=k>def</span> <span class=nf>display_superimposed_heatmaps</span><span class=p>(</span><span class=n>heatmaps</span><span class=p>,</span> <span class=n>image_path</span><span class=p>,</span> <span class=n>image_id</span><span class=p>):</span>
    <span class=n>n</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>heatmaps</span><span class=p>)</span>
    <span class=n>n_rows</span> <span class=o>=</span> <span class=p>(</span><span class=n>n</span> <span class=o>//</span> <span class=mi>3</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span> <span class=k>if</span> <span class=n>n</span> <span class=o>%</span> <span class=mi>3</span> <span class=o>&gt;</span> <span class=mi>0</span> <span class=k>else</span> <span class=n>n</span> <span class=o>//</span> <span class=mi>3</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>rcParams</span><span class=p>[</span><span class=s1>&#39;axes.grid&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=bp>False</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>rcParams</span><span class=p>[</span><span class=s1>&#39;xtick.labelsize&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=bp>False</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>rcParams</span><span class=p>[</span><span class=s1>&#39;ytick.labelsize&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=bp>False</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>rcParams</span><span class=p>[</span><span class=s1>&#39;xtick.top&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=bp>False</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>rcParams</span><span class=p>[</span><span class=s1>&#39;xtick.bottom&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=bp>False</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>rcParams</span><span class=p>[</span><span class=s1>&#39;ytick.left&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=bp>False</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>rcParams</span><span class=p>[</span><span class=s1>&#39;ytick.right&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=bp>False</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>rcParams</span><span class=p>[</span><span class=s1>&#39;figure.figsize&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=p>[</span><span class=mi>30</span><span class=p>,</span> <span class=mi>15</span><span class=p>]</span>
    <span class=k>for</span> <span class=n>index</span> <span class=ow>in</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>n</span><span class=p>):</span>
        <span class=n>heatmap</span> <span class=o>=</span> <span class=n>heatmaps</span><span class=p>[</span><span class=n>index</span><span class=p>][</span><span class=s2>&#34;heatmap&#34;</span><span class=p>]</span>
        <span class=n>class_id</span> <span class=o>=</span> <span class=n>heatmaps</span><span class=p>[</span><span class=n>index</span><span class=p>][</span><span class=s2>&#34;class_id&#34;</span><span class=p>]</span>
        <span class=n>class_name</span> <span class=o>=</span> <span class=n>CLASS_DICT</span><span class=p>[</span><span class=nb>str</span><span class=p>(</span><span class=n>class_id</span><span class=p>)]</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s2>&#34;,&#34;</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>capitalize</span><span class=p>()</span>
        <span class=n>superimposed_image</span> <span class=o>=</span> <span class=n>superimpose_heatmap</span><span class=p>(</span><span class=n>image_path</span><span class=p>,</span> <span class=n>heatmap</span><span class=p>)</span>
        <span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=n>n_rows</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>index</span><span class=o>+</span><span class=mi>1</span><span class=p>)</span>
        <span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=n>f</span><span class=s2>&#34;{class_id}, {class_name}&#34;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span> <span class=mi>30</span><span class=p>)</span>
        <span class=n>plt</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>superimposed_image</span><span class=p>)</span>
        
    <span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
<span class=n>display_superimposed_heatmaps</span><span class=p>(</span><span class=n>heatmaps</span><span class=p>,</span> <span class=n>IMG_PATH</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</code></pre></div><p><img src=output_38_0.png alt=png></p><h2 id=gradcam-for-classes-of-interest>GradCAM for Classes of Interest</h2><ul><li>For a predefined set of 6 classes we shall see the GradCAM efficacy</li><li>An image is carefully selected to view these predictions</li></ul><div class=highlight><pre class=chroma><code class=language-python data-lang=python>
<span class=n>display</span><span class=p>(</span><span class=n>Image</span><span class=p>(</span><span class=n>IMG_2_PATH</span><span class=p>))</span>
</code></pre></div><p><img src=output_40_0.jpg alt=jpeg></p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=c1>#Classes of Interest</span>
<span class=n>class_names</span> <span class=o>=</span> <span class=p>(</span><span class=s1>&#39;dock&#39;</span><span class=p>,</span> <span class=s1>&#39;pier&#39;</span><span class=p>,</span> <span class=s1>&#39;suspension_bridge&#39;</span><span class=p>,</span> <span class=s1>&#39;gondola&#39;</span><span class=p>,</span> <span class=s1>&#39;breakwater&#39;</span><span class=p>,</span> <span class=s1>&#39;dam&#39;</span><span class=p>)</span>
<span class=n>class_indices</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mi>536</span><span class=p>,</span> <span class=mi>718</span><span class=p>,</span> <span class=mi>839</span><span class=p>,</span> <span class=mi>576</span><span class=p>,</span> <span class=mi>460</span><span class=p>,</span> <span class=mi>525</span><span class=p>])</span>

<span class=n>classes</span> <span class=o>=</span> <span class=p>[(</span><span class=n>class_indices</span><span class=p>[</span><span class=n>index</span><span class=p>],</span> <span class=n>value</span><span class=p>)</span> <span class=k>for</span> <span class=n>index</span><span class=p>,</span> <span class=n>value</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>class_names</span><span class=p>)]</span>
<span class=n>classes</span>
</code></pre></div><pre><code>[(536, 'dock'),
 (718, 'pier'),
 (839, 'suspension_bridge'),
 (576, 'gondola'),
 (460, 'breakwater'),
 (525, 'dam')]
</code></pre><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=c1># Calculate Heatmaps for TOP_N Predictions</span>
<span class=n>heatmaps</span> <span class=o>=</span> <span class=n>make_gradcam_heatmap</span><span class=p>(</span>
    <span class=n>get_img_array</span><span class=p>(</span><span class=n>IMG_2_PATH</span><span class=p>,</span> <span class=n>IMG_SIZE</span><span class=p>),</span> 
    <span class=n>model_builder</span><span class=p>(</span><span class=n>weights</span><span class=o>=</span><span class=s2>&#34;imagenet&#34;</span><span class=p>),</span> 
    <span class=n>LAST_CONV_LAYER</span><span class=p>,</span> 
    <span class=n>CLASSIFIER_LAYER_NAMES</span><span class=p>,</span> 
    <span class=mi>0</span><span class=p>,</span> 
    <span class=n>class_indices</span>
<span class=p>)</span>
<span class=n>display_superimposed_heatmaps</span><span class=p>(</span><span class=n>heatmaps</span><span class=p>,</span> <span class=n>IMG_2_PATH</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</code></pre></div><p><img src=output_42_0.png alt=png></p><h3 id=reference>Reference</h3><p>Arxiv Paper Link: <a href=https://arxiv.org/abs/1610.02391>https://arxiv.org/abs/1610.02391</a><br><cite>@INPROCEEDINGS{8237336, author={R. R. {Selvaraju} and M. {Cogswell} and A. {Das} and R. {Vedantam} and D. {Parikh} and D. {Batra}}, booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}, title={Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization}, year={2017}, volume={}, number={}, pages={618-626},}</cite></p><div class=highlight><pre class=chroma><code class=language-python data-lang=python>
</code></pre></div></article></div><nav class="docs-toc d-none d-xl-block col-xl-4" aria-label="Secondary navigation"><div class=page-links><h3>On this page</h3><nav id=TableOfContents><ul><li><ul><li><a href=#objective>Objective</a></li><li><a href=#goal>Goal</a></li><li><a href=#is-this-topic-relevant-to-siim-isic-melanoma-detection>Is this topic relevant to SIIM-ISIC Melanoma Detection?</a></li><li><a href=#notebook-organization>Notebook Organization</a></li></ul></li></ul><ul><li><a href=#the-problem>The Problem.</a><ul><li><a href=#gradcam-architecture>GradCAM Architecture</a></li></ul></li><li><a href=#seeing-is-believing>Seeing is Believing.</a></li><li><a href=#why-grad-cam>Why Grad-CAM?</a></li><li><a href=#the-approach>The Approach</a></li><li><a href=#the-algorithm>The Algorithm.</a><ul><li><a href=#gradient-via-backpropagation>Gradient via Backpropagation</a></li><li><a href=#global-average-pooling-on-activation-maps>Global Average Pooling on Activation Maps</a></li><li><a href=#gradcam-activation-maps>GradCAM Activation Maps</a></li></ul></li></ul><ul><li><ul><li><a href=#vgg16-and-xception-properties>VGG16 and Xception Properties</a></li></ul></li><li><a href=#preprocessing>Preprocessing</a></li><li><a href=#make-gradcam-heatmap>Make GradCAM Heatmap</a><ul><li><a href=#steps>STEPS</a></li></ul></li><li><a href=#exploration-of-top-n-predictions>Exploration of Top N predictions</a></li><li><a href=#gradcam-for-classes-of-interest>GradCAM for Classes of Interest</a><ul><li><a href=#reference>Reference</a></li></ul></li></ul></nav></div></nav></div></div></div><footer class="footer text-muted"><div class=container><div class=row><div class="col-lg-8 order-last order-lg-first"><ul class=list-inline><li class=list-inline-item>Copyright 2021 <a href=https://www.gowrishankar.info/>GowriShankar.info</a> All rights reserved</li></ul></div><div class="col-lg-8 order-first order-lg-last text-lg-right"><ul class=list-inline></ul></div></div></div></footer><script src=https://www.gowrishankar.info/main.f6b484f556ad1f3bcf6061082139a2f21fa759f13930c39a25fe4a9f78f35e64122c2d86dffd56e67b292dabbda4095d8077194f196e0e348441c106a9f3d40e.js integrity="sha512-9rSE9VatHzvPYGEIITmi8h+nWfE5MMOaJf5Kn3jzXmQSLC2G3/1W5nspLau9pAldgHcZTxluDjSEQcEGqfPUDg==" crossorigin=anonymous defer></script><script src=https://www.gowrishankar.info/index.min.7a602ace1becaf60dc69027361aa7b13a225462f6639889a37f19953d6b8927cd3940da9251c0d016bebf2fb65357d9a23de4dd04e817f627f2902a88348b45c.js integrity="sha512-emAqzhvsr2DcaQJzYap7E6IlRi9mOYiaN/GZU9a4knzTlA2pJRwNAWvr8vtlNX2aI95N0E6Bf2J/KQKog0i0XA==" crossorigin=anonymous defer></script></body></html>