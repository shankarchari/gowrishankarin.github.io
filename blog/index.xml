<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Blog on</title><link>https://www.gowrishankar.info/blog/</link><description>Recent content in Blog on</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Tue, 06 Oct 2020 08:49:55 +0000</lastBuildDate><atom:link href="https://www.gowrishankar.info/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>Understanding Post-Synaptic Depression through Tsodyks-Markram Model by Solving Ordinary Differential Equation</title><link>https://www.gowrishankar.info/blog/understanding-post-synaptic-depression-through-tsodyks-markram-model-by-solving-ordinary-differential-equation/</link><pubDate>Fri, 12 Mar 2021 09:19:42 +0100</pubDate><guid>https://www.gowrishankar.info/blog/understanding-post-synaptic-depression-through-tsodyks-markram-model-by-solving-ordinary-differential-equation/</guid><description>This is an attempt to understand and run a commentary of TM Model proposed by Misha V.Tsodyks and Henry Markram in their 1997 paper named The neural code between neocortical pyramidal neurons depends on neurotransmitter release probability that demonstrates the complex computational tasks performed by the synapses in the neocortical pyramidal neuron pairs for a given stimuli.
Objective Objectives that we want to achieve by the end of this write</description></item><item><title>Automatic Differentiation Using Gradient Tapes</title><link>https://www.gowrishankar.info/blog/automatic-differentiation-using-gradient-tapes/</link><pubDate>Mon, 14 Dec 2020 09:19:42 +0100</pubDate><guid>https://www.gowrishankar.info/blog/automatic-differentiation-using-gradient-tapes/</guid><description>Introduction In this post, we shall deep dive into Tensorflow support for Differentiation using Gradient Tapes. Further explore the equations programmatically to understand the underlying capability.
In general differentiation can be accomplished through 4 techniques,
Manual Differentiation Numerical Differentiation (ND) Symbolic Differentiation (SD) Automatic Differentiation (AutoDiff) Both Numerical and Symbolic differentiations are considered as classical methods that are error prone. SD methods lead to inefficient code due to the challenges in converting a computer program into single expression.</description></item><item><title>Roll your sleeves! Let us do some partial derivatives.</title><link>https://www.gowrishankar.info/blog/roll-your-sleeves-let-us-do-some-partial-derivatives./</link><pubDate>Fri, 14 Aug 2020 09:19:42 +0100</pubDate><guid>https://www.gowrishankar.info/blog/roll-your-sleeves-let-us-do-some-partial-derivatives./</guid><description>from IPython.display import Image Forward and Backpropagation Algorithm An associated source code will be published shortly.
In this post, we shall explore a shallow Dense Neural Network(DNN) with
an Input Layer a Single Hidden Layer and an Output Layer The typical architecture of a feed forward network has the above units. Where
$x$ is a vector that represents the input values, $\hat y$ is a vector that represents the predictions and $h$ is vector that represents the hidden layer, $h$ is a hyperparameter selected based on the context of the problem Further to the above 3 parameters, connection weights $[W_1, W_2]$ and connection biases $[b_1, b_2]$ are the other matrices and vectors used in a DNN</description></item><item><title>GradCAM, Model Interpretability - VGG16 &amp; Xception Networks</title><link>https://www.gowrishankar.info/blog/gradcam-model-interpretability-vgg16-xception-networks/</link><pubDate>Sat, 04 Jul 2020 09:19:42 +0100</pubDate><guid>https://www.gowrishankar.info/blog/gradcam-model-interpretability-vgg16-xception-networks/</guid><description>Objective The objective of this post is to understand the importance of &amp;ldquo;Visual Explanations&amp;rdquo; for CNN based large scale Deep Neural Network Models.
I will be proud if one feels, this post is nothing but a commentary on R.R Selvaraju et al paper in IEEE ICCV, 2017 title Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization
Goal Decipher these Formulas and Program them using Keras and Numpy $$y_c = \frac{\partial{y^c}}{\partial{A^k}}$$</description></item><item><title>Tensorflow 2: Introduction, Feature Engineering and Metrics</title><link>https://www.gowrishankar.info/blog/tensorflow-2-introduction-feature-engineering-and-metrics/</link><pubDate>Sat, 04 Apr 2020 09:19:42 +0100</pubDate><guid>https://www.gowrishankar.info/blog/tensorflow-2-introduction-feature-engineering-and-metrics/</guid><description>Objective The key objective of this kernel is
To introduce the new programming style for creating a neural network using Keras on TF2 One should be capable of starting Deep Learning without going through the conventional way of ML algorithms and landing here eventually. Ref:Learn ML - ML101, Rank 4500 to ~450 in a day Instill knowledge on key principles includes Train, Validation and Test data splitting A simple mechanism to fill the missing values in the dataset Bias and Overfit handlers like class weights and initial bias calculation Handling categorical columns using One Hot Encoding or Embedding principles Elegant way of creating dataset of tensor slices from pandas dataframes Build a simple and flat a NN architecture using Keras Predict the targets via predict functions Analyse the results using various metrics include accuracy, precision, ROC curve etc This is an attempt to make an engineer novice to expert on approach and process of building a neural network for classification problem.</description></item><item><title>Time and Space Complexity - 5 Governing Rules</title><link>https://www.gowrishankar.info/blog/time-and-space-complexity-5-governing-rules/</link><pubDate>Fri, 28 Feb 2020 09:19:42 +0100</pubDate><guid>https://www.gowrishankar.info/blog/time-and-space-complexity-5-governing-rules/</guid><description>Story Behind It’s the job that’s never started as takes longest to finish. - Samwise Gamgee
Of late I noticed my folks are struggling on performance optimization while building a high throughput machine for a mission critical application. This notebook might give some idea on how to approach compute complexity and avoid obvious bottlenecks in an abstract fashion.
There is a good chance one might seek an external reference to comprehend this notebook fully.</description></item><item><title>ResNet50 vs InceptionV3 vs Xception vs NASNet - Introduction to Transfer Learning</title><link>https://www.gowrishankar.info/blog/resnet50-vs-inceptionv3-vs-xception-vs-nasnet-introduction-to-transfer-learning/</link><pubDate>Fri, 28 Jun 2019 09:19:42 +0100</pubDate><guid>https://www.gowrishankar.info/blog/resnet50-vs-inceptionv3-vs-xception-vs-nasnet-introduction-to-transfer-learning/</guid><description>Objective:
Objective of this kernel is to introduce transfer learning to beginners. I have taken the following deep neural network applications
ResNet50 InceptionV3 Xception NASNet Accuracy versus Computational Demand (Left) and Number of Parameters (Right) Transfer Learning Transfer learning is an ML methodology that enables to reuse a model developed for one task to another task. The applications are predominantly in Deep Learning for computer vision and natural language processing.</description></item></channel></rss>