<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=preload as=font href=https://shankarchari.github.io/gowrishankarin.github.io/fonts/vendor/jost/jost-v4-latin-regular.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=https://shankarchari.github.io/gowrishankarin.github.io/fonts/vendor/jost/jost-v4-latin-700.woff2 type=font/woff2 crossorigin><link rel=stylesheet href=https://shankarchari.github.io/gowrishankarin.github.io/main.cbe6761f5adb9ec38962b52f59fe61b74d7af20296267ab35a185e125109f79434987b0deb1827deedf23d9bcb7c554c77fa6052c01fa98769c7bb1939e3193c.css integrity="sha512-y+Z2H1rbnsOJYrUvWf5ht0168gKWJnqzWhheElEJ95Q0mHsN6xgn3u3yPZvLfFVMd/pgUsAfqYdpx7sZOeMZPA==" crossorigin=anonymous><noscript><style>img.lazyload{display:none}</style></noscript><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><title>Automatic Differentiation Using Gradient Tapes - Gowri Shankar</title><meta name=description content="As a Data Scientist or Deep Learning Researcher, one must have a deeper knowledge in various differentiation techniques due to the fact that gradient based optimization techniques like Backpropagation algorithms are critical for model efficiency and convergence."><link rel=canonical href=https://shankarchari.github.io/gowrishankarin.github.io/blog/automatic-differentiation-using-gradient-tapes/><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://shankarchari.github.io/gowrishankarin.github.io/profile.png"><meta name=twitter:title content="Automatic Differentiation Using Gradient Tapes"><meta name=twitter:description content="As a Data Scientist or Deep Learning Researcher, one must have a deeper knowledge in various differentiation techniques due to the fact that gradient based optimization techniques like Backpropagation algorithms are critical for model efficiency and convergence."><meta name=twitter:site content="@gowrishankarin"><meta name=twitter:creator content="@gowrishankarin"><meta property="og:title" content="Automatic Differentiation Using Gradient Tapes"><meta property="og:description" content="As a Data Scientist or Deep Learning Researcher, one must have a deeper knowledge in various differentiation techniques due to the fact that gradient based optimization techniques like Backpropagation algorithms are critical for model efficiency and convergence."><meta property="og:type" content="article"><meta property="og:url" content="https://shankarchari.github.io/gowrishankarin.github.io/blog/automatic-differentiation-using-gradient-tapes/"><meta property="og:image" content="https://shankarchari.github.io/gowrishankarin.github.io/profile.png"><meta property="article:published_time" content="2020-12-14T09:19:42+01:00"><meta property="article:modified_time" content="2021-03-03T09:19:42+01:00"><meta property="og:site_name" content="Gowri Shankar"><meta property="article:publisher" content="https://www.facebook.com/verlinde.henk"><meta property="article:author" content="https://www.facebook.com/verlinde.henk"><meta property="og:locale" content="en_US"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/shankarchari.github.io\/gowrishankarin.github.io\/blog\/automatic-differentiation-using-gradient-tapes\/"},"headline":"Automatic Differentiation Using Gradient Tapes","image":[],"datePublished":"2020-12-14T09:19:42CET","dateModified":"2021-03-03T09:19:42CET","author":{"@type":"Organization","name":"Gowri Shankar"},"publisher":{"@type":"Organization","name":"Gowri Shankar","logo":{"@type":"ImageObject","url":"https:\/\/shankarchari.github.io\/logo-doks.png"}},"description":"As a Data Scientist or Deep Learning Researcher, one must have a deeper knowledge in various differentiation techniques due to the fact that gradient based optimization techniques like Backpropagation algorithms are critical for model efficiency and convergence."}</script><script type=application/ld+json>{"@context":"http://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"https:\/\/shankarchari.github.io\/gowrishankarin.github.io\/"},{"@type":"ListItem","position":2,"name":"Blog","item":"https:\/\/shankarchari.github.io\/gowrishankarin.github.io\/blog\/"},{"@type":"ListItem","position":3,"name":"Automatic Differentiation Using Gradient Tapes","item":"https:\/\/shankarchari.github.io\/gowrishankarin.github.io\/blog\/automatic-differentiation-using-gradient-tapes\/"}]}</script><meta name=theme-color content="#fff"><link rel=apple-touch-icon sizes=180x180 href=https://shankarchari.github.io/gowrishankarin.github.io/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://shankarchari.github.io/gowrishankarin.github.io/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://shankarchari.github.io/gowrishankarin.github.io/favicon-16x16.png><link rel=manifest href=https://shankarchari.github.io/gowrishankarin.github.io/site.webmanifest><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
</script></head><body class="blog single"><div class="header-bar fixed-top"></div><header class="navbar fixed-top navbar-expand-md navbar-light"><div class=container><input class="menu-btn order-0" type=checkbox id=menu-btn>
<label class="menu-icon d-md-none" for=menu-btn><span class=navicon></span></label><a class="navbar-brand order-1 order-md-0 mr-auto" href=https://shankarchari.github.io/gowrishankarin.github.io/>Gowri Shankar</a>
<button id=mode class="btn btn-link order-2 order-md-4" type=button aria-label="Toggle mode">
<span class=toggle-dark><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></span><span class=toggle-light><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></span></button><ul class="navbar-nav social-nav order-3 order-md-5"><li class=nav-item><a class=nav-link href=https://github.com/gowrishankarin><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77a5.44 5.44.0 00-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg><span class="ml-2 sr-only">GitHub</span></a></li></ul><div class="collapse navbar-collapse order-4 order-md-1"><ul class="navbar-nav main-nav mr-auto order-5 order-md-2"><li class=nav-item><a class=nav-link href=https://shankarchari.github.io/gowrishankarin.github.io/docs/prologue/education/>Accolades</a></li><li class="nav-item active"><a class=nav-link href=https://shankarchari.github.io/gowrishankarin.github.io/blog/>Blog</a></li><li class=nav-item><a class=nav-link href=https://shankarchari.github.io/gowrishankarin.github.io/reads/>Bookshelf</a></li><li class=nav-item><a class=nav-link href=https://shankarchari.github.io/gowrishankarin.github.io/contributors/gowri-shankar>Contact</a></li></ul><div class="break order-6 d-md-none"></div><form class="navbar-form flex-grow-1 order-7 order-md-3"><input id=userinput class="form-control is-search" type=search placeholder="Search docs..." aria-label="Search docs..." autocomplete=off><div id=suggestions class="shadow bg-white rounded"></div></form></div></div></header><div class="wrap container" role=document><div class=content><div class="row justify-content-left"><div class=col-12><article><div class=blog-header><h1>Automatic Differentiation Using Gradient Tapes</h1><p><small>Posted December 14, 2020 by <a class="stretched-link position-relative" href=https://shankarchari.github.io/contributors/gowri-shankar/>Gowri Shankar</a>&nbsp;&dash;&nbsp;<strong>9&nbsp;min read</strong></small><p></div><p class=lead>As a Data Scientist or Deep Learning Researcher, one must have a deeper knowledge in various differentiation techniques due to the fact that gradient based optimization techniques like Backpropagation algorithms are critical for model efficiency and convergence.</p><h2 id=introduction>Introduction</h2><p>In this post, we shall deep dive into Tensorflow support for Differentiation using Gradient Tapes. Further explore the equations programmatically to understand the underlying capability.</p><p>In general differentiation can be accomplished through 4 techniques,</p><ul><li>Manual Differentiation</li><li>Numerical Differentiation (ND)</li><li>Symbolic Differentiation (SD)</li><li>Automatic Differentiation (AutoDiff)</li></ul><p>Both Numerical and Symbolic differentiations are considered as classical methods that are error prone. SD methods lead to inefficient code due to the challenges in converting a computer program into single expression. Meanwhile Numerical methods introduce round of errors due to discretization process(limits). Further, they are not suitable for gradient descent which is the backbone of Backpropation due to their inefficiency and performance bottleneck in computing partial derivatives.</p><p><img src=auto_diff.png alt=png></p><ul><li>Train inputs $x_i$ are fed forward, generating corresponding activations $y_i$</li><li>An error E between the output $y_3$ and the target output $t$ is computed</li><li>The error adjoint is propagated backward, giving the gradient with respect to the weights<br>$\bigtriangledown_{wi}E = \left( \frac{\partial E}{\partial w_1}, \cdots, \frac {\partial E}{\partial w_6} \right)$<br>which is subsequently used in a gradient-descent procedure</li><li>The gradient wrt inputs $\bigtriangledown_{wi}E$ also be computed in the same backward pass</li></ul><p>Image and Overview of Backpropagation Reference: <a href=https://arxiv.org/abs/1502.05767>Automatic Differentiation
in Machine Learning: a Survey</a></p><h2 id=goal>Goal</h2><p>I would consider the goal is accomplished, If one find answer to the following questions.</p><ul><li>What is Automatic Differentiation</li><li>Where it is used?</li><li>What is chain rule?</li><li>How to find unknown differentiable equation from data?</li><li>How to compute gradients?</li><li>What is the significance of Gradient Tapes?</li><li>Beyond equations, a programmatical implementation of the Auto Diff for few examples</li></ul><h2 id=chain-rule>Chain Rule</h2><p>Chain rule is a formula to compute derivative of a composition of differentiable functions. To get an intuition, a differentiable function has a graph that is smooth and does not contain any break, angle or cusp.
Chain rule is often confused due to its application both in <code>Calculus</code> and <code>Probability</code>(joint distribution, conditional probability and Bayesian networks).</p><p>$$\Large y = f(g(h(x)))) = f(g(h(w_{0}))) = f(g(w_{1})) = f(w_{2}) = w_{3}$$</p><p>Where, $w_0 = x$, $w_1 = h(w_0)$, $w_2 = g(w_1)$, $w_3 = f(w_2) = y$</p><p>then the chain rule gives<br>$\Large \frac{dy}{dx} = \frac{dy}{dw_2} \frac{dw_2}{dw_1} \frac{dw_1}{dx}$<br>$i.e$<br>$\Large \frac{dy}{dx} = \frac{df(w_2)}{dw_2} \frac{dg(w_1)}{dw_1} \frac{dh(w_0)}{dx}$</p><h3 id=accumulations-methods>Accumulations Methods</h3><p>There are two modes through which AutoDiff is performed,</p><ol><li>Forward Accumuation and</li><li>Backward Accumuation - Used in Backpropagation of errors in Multi Layer Perceptron Deep Neural Networks</li></ol><p>An equation worth millions compared to explanation of any kind in this quest.</p><h4 id=forward-accumulation>Forward Accumulation</h4><p>In forward accumulation, chain rule traverses from inside to outside<br>$\Large \frac{\partial y}{\partial x} = \frac{\partial y}{\partial w_{n-1}} \frac{\partial w_{n-1}}{dw_x}$</p><p>$\Large \frac{\partial y}{\partial x} = \frac{\partial y}{\partial w_{n-1}} \left(\frac{\partial w_{n-1}}{dw_{n-2}} \frac{\partial w_{n-2}}{dw_x} \right)$</p><p>$\Large \frac{\partial y}{\partial x} = \frac{\partial y}{\partial w_{n-1}} \left(\frac{\partial w_{n-1}}{dw_{n-2}} \left(\frac{\partial w_{n-2}}{dw_{n-3}} \frac{\partial w_{n-3}}{dw_x} \right) \right) = \dots$</p><h4 id=reverse-accumulation>Reverse Accumulation</h4><p>In reverse accumulation, chain rule traverses from outside to inside</p><p>$\Large \frac{\partial y}{\partial x} = \frac{\partial y}{\partial w_1} \frac{\partial w_1}{dw_x}$</p><p>$\Large \frac{\partial y}{\partial x} = \left( \frac{\partial y}{\partial w_2} \frac{\partial w_2}{dw_1} \right) \frac{\partial w_1}{dw_x} $</p><p>$\Large \frac{\partial y}{\partial x} = \left( \left( \frac{\partial y}{\partial w_3}\frac{\partial w_3}{dw_2}\right) \frac{\partial w_2}{dw_1} \right) \frac{\partial w_1}{dw_x} = \dots$</p><h2 id=automatic-differentiation-using-tensorflow>Automatic Differentiation using Tensorflow</h2><p>We saw what is a differential equation, we also pondered the salience of Backward accumulation in Multi Layer Perceptrons. Our goal for a <code>Deep Neural Network(DNN)</code> model is to minimize the error of an unknown differentiable equation. What is this unknown differentiable equation is a separate topic for an other day.</p><p>Minimizing the error is achieved by finding a <code>local minima</code> of the differentiable equation iteratively using an optimization algorithm. This process is called as <code>Gradient Descent</code>. For few other problems, we might change the direction and find the <code>local maxima</code> and that process is called as <code>Gradient Ascent</code>.</p><p>During DNN training, two operations occurs <code>Forward Pass</code> and <code>Backward Pass</code>. To differentiate automatically, one need to remember what happened during the <em>forward pass</em> and while traversing back these operations happened are reversed to compute gradients. Tensorflow provides Gradient tapes to remember, A right analogy is like our olden day VHS tapes. Things are recorded at every step of training and can be reversed just by traversing back.</p><h3 id=gradient-tapes>Gradient Tapes</h3><p>Tensorflow provided <code>tf.GradientTape</code> API for automatic differentiation to compute the gradient of certain inputs by recording the operations executed inside certain context. We shall examine this with few examples</p><h3 id=calculate-derivatives>Calculate Derivatives</h3><p>Let us see, how to calculate derivative for this simple equation
$$\frac{d}{dx}x^3 =3x^2$$
$$f'(x=1) = 3$$
$$f'(x=3) = 27$$</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>numpy</span> <span class=kn>as</span> <span class=nn>np</span>
<span class=kn>import</span> <span class=nn>tensorflow</span> <span class=kn>as</span> <span class=nn>tf</span>

<span class=k>def</span> <span class=nf>derivative</span><span class=p>(</span><span class=n>value</span><span class=p>):</span>
    <span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>([[</span><span class=n>value</span><span class=p>]])</span>
    <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>GradientTape</span><span class=p>()</span> <span class=k>as</span> <span class=n>tape</span><span class=p>:</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>x</span> <span class=o>*</span> <span class=n>x</span> <span class=o>*</span> <span class=n>x</span>
        
        <span class=n>dy_dx</span> <span class=o>=</span> <span class=n>tape</span><span class=o>.</span><span class=n>gradient</span><span class=p>(</span><span class=n>loss</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>

    <span class=k>return</span> <span class=n>dy_dx</span>
</code></pre></div><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>derive_1</span> <span class=o>=</span> <span class=n>derivative</span><span class=p>(</span><span class=mf>1.0</span><span class=p>)</span>
<span class=n>derive_3</span> <span class=o>=</span> <span class=n>derivative</span><span class=p>(</span><span class=mf>3.0</span><span class=p>)</span>
</code></pre></div><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>print</span><span class=p>(</span><span class=n>f</span><span class=s1>&#39;f</span><span class=se>\&#39;</span><span class=s1>(𝑥=1) = {derive_1.numpy()}&#39;</span><span class=p>)</span>
<span class=k>print</span><span class=p>(</span><span class=n>f</span><span class=s1>&#39;f</span><span class=se>\&#39;</span><span class=s1>(𝑥=3) = {derive_3.numpy()}&#39;</span><span class=p>)</span>
</code></pre></div><pre><code>f'(𝑥=1) = [[3.]]
f'(𝑥=3) = [[27.]]
</code></pre><h3 id=calculate-derivative-by-partial-for-higher-rank-matrix>Calculate Derivative by Partial for Higher Rank Matrix</h3><p>Let us say, We have a matrix of shape $2 \times 2$. That can be represented as $eqn.2$.
We want to find the derivative of the square of that matrix. This is best done using partial derivative by assigning to a third variable $z$ ref $eqn.2$
$$y = \sum x$$
$$y=x_{1,1} + x_{1,2} + x_{2,1} + x_{2,2} \tag{1}$$
$$z=y^2\tag{2}$$</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>ones</span><span class=p>((</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
<span class=n>x</span>
</code></pre></div><pre><code>&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[1., 1.],
       [1., 1.]], dtype=float32)&gt;
</code></pre><p>From the chain rule</p><p>$$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \times \frac{\partial y}{\partial x}$$
from equation 2
$$\frac{\partial z}{\partial y} = 2 \times y = 8$$
from equation 1
$$\frac{\partial y}{\partial x}= \frac{\partial y}{\partial x_{1,1}}, \frac{\partial y}{\partial x_{1,2}}, \frac{\partial y}{\partial x_{2,1}}, \frac{\partial y}{\partial x_{2,2}}$$
$$\frac{\partial y}{\partial x}= [[1, 1], [1, 1]] $$
hence
$$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \times \frac{\partial y}{\partial x}$$
$$\frac{\partial z}{\partial x} = 8 \times [[1, 1], [1, 1]] = [[8, 8], [8, 8]]$$</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>GradientTape</span><span class=p>()</span> <span class=k>as</span> <span class=n>tape</span><span class=p>:</span>
    <span class=n>tape</span><span class=o>.</span><span class=n>watch</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
    <span class=n>y</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
    <span class=n>z</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>square</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
    
<span class=n>dz_dx</span> <span class=o>=</span> <span class=n>tape</span><span class=o>.</span><span class=n>gradient</span><span class=p>(</span><span class=n>z</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>
</code></pre></div><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>dz_dx</span>
</code></pre></div><pre><code>&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[8., 8.],
       [8., 8.]], dtype=float32)&gt;
</code></pre><h2 id=derivatives-for-higher-degree-polynomial-equation>Derivatives for Higher Degree Polynomial Equation</h2><p>Let $x = 2$ and the 2nd degree polynomial equations as follows.</p><p>$$y = x^2 \tag{3}$$
$$z = y^2 \tag{4}$$</p><p>Using chain rule, Let us solve the derivative</p><p>$$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \times \frac{\partial y}{\partial x}$$
from equation 2
$$\frac{\partial z}{\partial y} = 2 \times y = 2 \times 3^2 = 18$$
$$\frac{\partial y}{\partial x} = 2x = 2 \times 3 = 6 \tag{5}$$
hence
$$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \times \frac{\partial y}{\partial x} = 18 \times 6 = 108 \tag{6}$$</p><p><em>Solve the same by substitution</em>
$$\frac{\partial z}{\partial x}= \frac{\partial}{\partial x}x^4$$
$$\frac{\partial z}{\partial x}= 4x^3 = 4 \times 3^3 = 108 $$</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>(</span><span class=mf>3.0</span><span class=p>)</span>
<span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>GradientTape</span><span class=p>(</span><span class=n>persistent</span><span class=o>=</span><span class=bp>True</span><span class=p>)</span> <span class=k>as</span> <span class=n>tape</span><span class=p>:</span>
    <span class=n>tape</span><span class=o>.</span><span class=n>watch</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
    <span class=n>y</span> <span class=o>=</span> <span class=n>x</span> <span class=o>**</span> <span class=mi>2</span>
    <span class=n>z</span> <span class=o>=</span> <span class=n>y</span> <span class=o>**</span> <span class=mi>2</span>
    
<span class=n>dz_dx</span> <span class=o>=</span> <span class=n>tape</span><span class=o>.</span><span class=n>gradient</span><span class=p>(</span><span class=n>z</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>
<span class=n>dy_dx</span> <span class=o>=</span> <span class=n>tape</span><span class=o>.</span><span class=n>gradient</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>
</code></pre></div><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>print</span><span class=p>(</span><span class=n>f</span><span class=s1>&#39;∂𝑧/∂𝑥: {dz_dx}, ∂𝑦/∂𝑥: {dy_dx}&#39;</span><span class=p>)</span>
</code></pre></div><pre><code>∂𝑧/∂𝑥: 108.0, ∂𝑦/∂𝑥: 6.0
</code></pre><h3 id=derivative-of-derivative>Derivative of Derivative</h3><p>$$y=x^3$$
$$\frac{\partial y}{\partial x} = 3x^2$$
$$\frac{\partial^2 y}{\partial x^2}=6x$$</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>derivative_of_derivative</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
    <span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
    <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>GradientTape</span><span class=p>()</span> <span class=k>as</span> <span class=n>tape_2</span><span class=p>:</span>
        <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>GradientTape</span><span class=p>()</span> <span class=k>as</span> <span class=n>tape_1</span><span class=p>:</span>
            <span class=n>y</span> <span class=o>=</span> <span class=n>x</span> <span class=o>**</span> <span class=mi>3</span>
        <span class=n>dy_dx</span> <span class=o>=</span> <span class=n>tape_1</span><span class=o>.</span><span class=n>gradient</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>
    <span class=n>d2y_dx2</span> <span class=o>=</span> <span class=n>tape_2</span><span class=o>.</span><span class=n>gradient</span><span class=p>(</span><span class=n>dy_dx</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>
    
    <span class=k>return</span> <span class=n>dy_dx</span><span class=p>,</span> <span class=n>d2y_dx2</span>
</code></pre></div><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>dy_dx</span><span class=p>,</span> <span class=n>d2y_dx2</span> <span class=o>=</span> <span class=n>derivative_of_derivative</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
<span class=k>print</span><span class=p>(</span><span class=n>f</span><span class=s1>&#39;x=1 -- ∂𝑦/∂𝑥: {dy_dx}, ∂2𝑦∂𝑥2: {d2y_dx2}&#39;</span><span class=p>)</span>

<span class=n>dy_dx</span><span class=p>,</span> <span class=n>d2y_dx2</span> <span class=o>=</span> <span class=n>derivative_of_derivative</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span>
<span class=k>print</span><span class=p>(</span><span class=n>f</span><span class=s1>&#39;x=5 -- ∂𝑦/∂𝑥: {dy_dx}, ∂2𝑦∂𝑥2: {d2y_dx2}&#39;</span><span class=p>)</span>
</code></pre></div><pre><code>x=1 -- ∂𝑦/∂𝑥: 3.0, ∂2𝑦∂𝑥2: 6.0
x=5 -- ∂𝑦/∂𝑥: 75.0, ∂2𝑦∂𝑥2: 30.0
</code></pre><h2 id=finding-unknown-differentiable-equation>Finding Unknown Differentiable Equation</h2><p>Let us extract this function programmatically using <code>tf.GradientTapes</code></p><p>$$y = 2x - 8 $$
$$\frac{dy}{dx} = 2 $$</p><p>Let us recreate this using <code>numpy</code> and <code>tensorflow</code> gradient tapes</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>(</span><span class=mi>15</span><span class=p>,</span> <span class=n>size</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>replace</span><span class=o>=</span><span class=bp>False</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=nb>float</span><span class=p>)</span>
<span class=n>y</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>x</span> <span class=o>-</span> <span class=mi>8</span>
<span class=k>print</span><span class=p>(</span><span class=n>f</span><span class=s2>&#34;x: {x}</span><span class=se>\n</span><span class=s2>y: {y}&#34;</span><span class=p>)</span>
</code></pre></div><pre><code>x: [ 8. 10.  6.  3.  7.  9. 13.  1.  0. 11.]
y: [ 8. 12.  4. -2.  6. 10. 18. -6. -8. 14.]
</code></pre><p>We have the data pattern for the above said equation. Through this dataset, let us find the equation by performing following steps</p><ul><li>The above equation is of the form $y = mx + b$, each instance is a point on the 2D plane that connects and forms a line
$$y = mx + b$$</li><li>We have to train the variable $m$ the coefficient and $b$ the intercept. $m$ and $b$ using <code>tf.Variable</code></li><li>As mentioned above, our goal is the minimize the error by using an optimization function. A <em>loss</em>($loss_{fn}$) function using <code>tf.abs</code> of predicted $\hat y$ and actual $y$</li><li>We have to compute this through multiple iterations. A <em>fit</em> functions using <code>tf.GradientTape</code> by iterating for <code>EPOCHS</code> time</li><li>We also have a <code>LEARNING RATE</code> that is a constant</li></ul><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>LEARNING_RATE</span> <span class=o>=</span> <span class=mf>0.001</span>
<span class=n>m</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>random</span><span class=p>(),</span> <span class=n>trainable</span><span class=o>=</span><span class=bp>True</span><span class=p>)</span>
<span class=n>c</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>random</span><span class=p>(),</span> <span class=n>trainable</span><span class=o>=</span><span class=bp>True</span><span class=p>)</span>

<span class=k>def</span> <span class=nf>loss_fn</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>y_hat</span><span class=p>):</span>
    <span class=k>return</span> <span class=n>tf</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>y</span> <span class=o>-</span> <span class=n>y_hat</span><span class=p>)</span>

<span class=k>def</span> <span class=nf>fit</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
    <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>GradientTape</span><span class=p>(</span><span class=n>persistent</span><span class=o>=</span><span class=bp>True</span><span class=p>)</span> <span class=k>as</span> <span class=n>tape</span><span class=p>:</span>
        <span class=c1># Predict y from data</span>
        <span class=n>y_hat</span> <span class=o>=</span> <span class=n>m</span> <span class=o>*</span> <span class=n>x</span> <span class=o>+</span> <span class=n>c</span>
        
        <span class=c1># Calculate the loss</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>loss_fn</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>y_hat</span><span class=p>)</span>
        
    <span class=c1># Calculate Gradients</span>
    <span class=n>m_gradient</span> <span class=o>=</span> <span class=n>tape</span><span class=o>.</span><span class=n>gradient</span><span class=p>(</span><span class=n>loss</span><span class=p>,</span> <span class=n>m</span><span class=p>)</span>
    <span class=n>c_gradient</span> <span class=o>=</span> <span class=n>tape</span><span class=o>.</span><span class=n>gradient</span><span class=p>(</span><span class=n>loss</span><span class=p>,</span> <span class=n>c</span><span class=p>)</span>
    
    <span class=c1># Update the Gradient and apply learning </span>
    <span class=n>m</span><span class=o>.</span><span class=n>assign_sub</span><span class=p>(</span><span class=n>m_gradient</span> <span class=o>*</span> <span class=n>LEARNING_RATE</span><span class=p>)</span>
    <span class=n>c</span><span class=o>.</span><span class=n>assign_sub</span><span class=p>(</span><span class=n>c_gradient</span> <span class=o>*</span> <span class=n>LEARNING_RATE</span><span class=p>)</span>
</code></pre></div><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>EPOCHS</span> <span class=o>=</span> <span class=mi>2000</span>
<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>EPOCHS</span><span class=p>):</span>
    <span class=n>fit</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
    <span class=k>if</span><span class=p>(</span><span class=n>i</span> <span class=o>%</span> <span class=mi>100</span> <span class=o>==</span> <span class=mi>0</span><span class=p>):</span>
        <span class=k>print</span><span class=p>(</span><span class=n>f</span><span class=s1>&#39;EPOCH: {i:04d}, m: {m.numpy()}, c: {c.numpy()}&#39;</span><span class=p>)</span>
</code></pre></div><pre><code>EPOCH: 0000, m: 0.6082303524017334, c: 0.19983896613121033
EPOCH: 0100, m: 1.1402301788330078, c: -0.1361609399318695
EPOCH: 0200, m: 1.1762301921844482, c: -0.532161295413971
EPOCH: 0300, m: 1.2302302122116089, c: -0.9261620044708252
EPOCH: 0400, m: 1.2662302255630493, c: -1.3221579790115356
EPOCH: 0500, m: 1.3022302389144897, c: -1.7181528806686401
EPOCH: 0600, m: 1.3562302589416504, c: -2.1121480464935303
EPOCH: 0700, m: 1.3922302722930908, c: -2.508143186569214
EPOCH: 0800, m: 1.4462302923202515, c: -2.9021384716033936
EPOCH: 0900, m: 1.482230305671692, c: -3.298133611679077
EPOCH: 1000, m: 1.5362303256988525, c: -3.692128896713257
EPOCH: 1100, m: 1.572230339050293, c: -4.08812952041626
EPOCH: 1200, m: 1.6262303590774536, c: -4.482147216796875
EPOCH: 1300, m: 1.662230372428894, c: -4.878165245056152
EPOCH: 1400, m: 1.6982303857803345, c: -5.27418327331543
EPOCH: 1500, m: 1.7522304058074951, c: -5.668200969696045
EPOCH: 1600, m: 1.7882304191589355, c: -6.064218997955322
EPOCH: 1700, m: 1.8402304649353027, c: -6.458236217498779
EPOCH: 1800, m: 1.884230613708496, c: -6.8522515296936035
EPOCH: 1900, m: 1.9182307720184326, c: -7.246265888214111
</code></pre><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>print</span><span class=p>(</span><span class=n>f</span><span class=s2>&#34;y ~ {m.numpy()}x + {c.numpy()}&#34;</span><span class=p>)</span>
</code></pre></div><pre><code>y ~ 1.9022306203842163x + -7.554266452789307
</code></pre><p>Hence,
$$\Huge y \sim 1.92x - 7.22$$
$$\Huge \simeq$$
$$\Huge y = 2x - 8 $$</p><h2 id=inference>Inference</h2><ol><li>Have we found the polynomial function programmatically using Gradient Tapes? - YES</li><li>Are we able to differentiate polynomial equation of various degrees? - YES</li><li>Are we able to compute gradeints using Gradient Tape? - Yes</li><li>Have we established the relationship between Chain Rule and Gradient Descent? - YES</li><li>Have we achieved our goals? - If YES, I request you to promote this blog by a tweet or a linkedin share. It means a lot to me.</li></ol><p>Thank You</p><h2 id=references>References</h2><blockquote><p><a href=https://www.tensorflow.org/guide/autodiff>Automatic Differentiation and Gradients</a><br><a href=http://www.math.uri.edu/~mcomerford/math141/Fall11/lesson13.pdf>Differentiable Functions</a><br><a href=https://arxiv.org/pdf/1502.05767.pdf>Automatic Differentiation in Machine Learning: a Survey</a></p></blockquote><div class=highlight><pre class=chroma><code class=language-python data-lang=python>
</code></pre></div></article></div><nav class="docs-toc d-none d-xl-block col-xl-4" aria-label="Secondary navigation"><div class=page-links><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#goal>Goal</a></li><li><a href=#chain-rule>Chain Rule</a><ul><li><a href=#accumulations-methods>Accumulations Methods</a></li></ul></li><li><a href=#automatic-differentiation-using-tensorflow>Automatic Differentiation using Tensorflow</a><ul><li><a href=#gradient-tapes>Gradient Tapes</a></li><li><a href=#calculate-derivatives>Calculate Derivatives</a></li><li><a href=#calculate-derivative-by-partial-for-higher-rank-matrix>Calculate Derivative by Partial for Higher Rank Matrix</a></li></ul></li><li><a href=#derivatives-for-higher-degree-polynomial-equation>Derivatives for Higher Degree Polynomial Equation</a><ul><li><a href=#derivative-of-derivative>Derivative of Derivative</a></li></ul></li><li><a href=#finding-unknown-differentiable-equation>Finding Unknown Differentiable Equation</a></li><li><a href=#inference>Inference</a></li><li><a href=#references>References</a></li></ul></nav></div></nav></div></div></div><footer class="footer text-muted"><div class=container><div class=row><div class="col-lg-8 order-last order-lg-first"><ul class=list-inline><li class=list-inline-item>Copyright 2021 <a href=https://www.gowrishankar.info/>GowriShankar.info</a> All rights reserved</li></ul></div><div class="col-lg-8 order-first order-lg-last text-lg-right"><ul class=list-inline></ul></div></div></div></footer><script src=https://shankarchari.github.io/gowrishankarin.github.io/main.f6b484f556ad1f3bcf6061082139a2f21fa759f13930c39a25fe4a9f78f35e64122c2d86dffd56e67b292dabbda4095d8077194f196e0e348441c106a9f3d40e.js integrity="sha512-9rSE9VatHzvPYGEIITmi8h+nWfE5MMOaJf5Kn3jzXmQSLC2G3/1W5nspLau9pAldgHcZTxluDjSEQcEGqfPUDg==" crossorigin=anonymous defer></script><script src=https://shankarchari.github.io/gowrishankarin.github.io/index.min.e8052e2ce1e90dd5f40dd0d369a12cad1ce7a9a56906d0ad60aa3501d7e82108ec4ecdf859f27fe831d0a878df325860e1813692f8741b516d3704d939317965.js integrity="sha512-6AUuLOHpDdX0DdDTaaEsrRznqaVpBtCtYKo1AdfoIQjsTs34WfJ/6DHQqHjfMlhg4YE2kvh0G1FtNwTZOTF5ZQ==" crossorigin=anonymous defer></script></body></html>