<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=preload as=font href=https://www.gowrishankar.info/fonts/vendor/jost/jost-v4-latin-regular.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=https://www.gowrishankar.info/fonts/vendor/jost/jost-v4-latin-700.woff2 type=font/woff2 crossorigin><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin=anonymous><link rel=stylesheet href=https://www.gowrishankar.info/main.81f6f29d305feeb7d67df851e890d245078d15aa4ee2077b34695f7dc9d39705614b8a4f3ec23585a04a51eafe279f1a9114222f5e8fa2ad9f371def65df9b89.css integrity="sha512-gfbynTBf7rfWffhR6JDSRQeNFapO4gd7NGlffcnTlwVhS4pPPsI1haBKUer+J58akRQiL16Poq2fNx3vZd+biQ==" crossorigin=anonymous><noscript><style>img.lazyload{display:none}</style></noscript><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><title>Roll your sleeves! Let us do some partial derivatives. - Gowri Shankar</title><meta name=description content="In this post, we shall explore a shallow neural network with a single hidden layer and the math behind back propagation algorithm, gradient descent"><link rel=canonical href=https://www.gowrishankar.info/blog/roll-your-sleeves-let-us-do-some-partial-derivatives./><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.gowrishankar.info/blog/roll-your-sleeves-let-us-do-some-partial-derivatives./output_3_0.png"><meta name=twitter:title content="Roll your sleeves! Let us do some partial derivatives."><meta name=twitter:description content="In this post, we shall explore a shallow neural network with a single hidden layer and the math behind back propagation algorithm, gradient descent"><meta name=twitter:site content="@gowrishankarin"><meta name=twitter:creator content="@gowrishankarin"><meta property="og:title" content="Roll your sleeves! Let us do some partial derivatives."><meta property="og:description" content="In this post, we shall explore a shallow neural network with a single hidden layer and the math behind back propagation algorithm, gradient descent"><meta property="og:type" content="article"><meta property="og:url" content="https://www.gowrishankar.info/blog/roll-your-sleeves-let-us-do-some-partial-derivatives./"><meta property="og:image" content="https://www.gowrishankar.info/blog/roll-your-sleeves-let-us-do-some-partial-derivatives./output_3_0.png"><meta property="article:published_time" content="2020-08-14T09:19:42+01:00"><meta property="article:modified_time" content="2021-02-28T09:19:42+01:00"><meta property="og:site_name" content="Gowri Shankar"><meta property="article:publisher" content="https://www.facebook.com/verlinde.henk"><meta property="article:author" content="https://www.facebook.com/verlinde.henk"><meta property="og:locale" content="en_US"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/www.gowrishankar.info\/blog\/roll-your-sleeves-let-us-do-some-partial-derivatives.\/"},"headline":"Roll your sleeves! Let us do some partial derivatives.","image":["https://www.gowrishankar.info/blog/roll-your-sleeves-let-us-do-some-partial-derivatives./output_3_0.png"],"datePublished":"2020-08-14T09:19:42CET","dateModified":"2021-02-28T09:19:42CET","author":{"@type":"Organization","name":"Gowri Shankar"},"publisher":{"@type":"Organization","name":"Gowri Shankar","logo":{"@type":"ImageObject","url":"https:\/\/www.gowrishankar.info\/logo-doks.png"}},"description":"In this post, we shall explore a shallow neural network with a single hidden layer and the math behind back propagation algorithm, gradient descent"}</script><script type=application/ld+json>{"@context":"http://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"https:\/\/www.gowrishankar.info\/"},{"@type":"ListItem","position":2,"name":"Blog","item":"https:\/\/www.gowrishankar.info\/blog\/"},{"@type":"ListItem","position":3,"name":"Roll Your Sleeves Let Us Do Some Partial Derivatives.","item":"https:\/\/www.gowrishankar.info\/blog\/roll-your-sleeves-let-us-do-some-partial-derivatives.\/"}]}</script><meta name=theme-color content="#fff"><link rel=apple-touch-icon sizes=180x180 href=https://www.gowrishankar.info/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://www.gowrishankar.info/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://www.gowrishankar.info/favicon-16x16.png><link rel=manifest href=https://www.gowrishankar.info/site.webmanifest><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-K84DCXJ');</script><script src=https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/components/prism-core.min.js integrity="sha512-xR+IAyN+t9EBIOOJw5m83FTVMDsPd63IhJ3ElP4gmfUFnQlX9+eWGLp3P4t3gIjpo2Z1JzqtW/5cjgn+oru3yQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/plugins/autoloader/prism-autoloader.min.js integrity="sha512-zc7WDnCM3aom2EziyDIRAtQg1mVXLdILE09Bo+aE1xk0AM2c2cVLfSW9NrxE5tKTX44WBY0Z2HClZ05ur9vB6A==" crossorigin=anonymous></script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
</script></head><body class="blog single"><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-K84DCXJ" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><div class="header-bar fixed-top"></div><header class="navbar fixed-top navbar-expand-md navbar-light"><div class=container><input class="menu-btn order-0" type=checkbox id=menu-btn>
<label class="menu-icon d-md-none" for=menu-btn><span class=navicon></span></label><a class="navbar-brand order-1 order-md-0 mr-auto" href=https://www.gowrishankar.info/>Gowri Shankar</a>
<button id=mode class="btn btn-link order-2 order-md-4" type=button aria-label="Toggle mode">
<span class=toggle-dark><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></span><span class=toggle-light><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></span></button><ul class="navbar-nav social-nav order-3 order-md-5"><li class=nav-item><a class=nav-link href=https://github.com/gowrishankarin><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77a5.44 5.44.0 00-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg><span class="ml-2 sr-only">GitHub</span></a></li></ul><div class="collapse navbar-collapse order-4 order-md-1"><ul class="navbar-nav main-nav mr-auto order-5 order-md-2"><li class=nav-item><a class=nav-link href=https://www.gowrishankar.info/docs/prologue/education/>Accolades</a></li><li class="nav-item active"><a class=nav-link href=https://www.gowrishankar.info/blog/>Blog</a></li><li class=nav-item><a class=nav-link href=https://www.gowrishankar.info/reads/>Bookshelf</a></li><li class=nav-item><a class=nav-link href=https://www.gowrishankar.info/contributors/gowri-shankar>Contact</a></li></ul><div class="break order-6 d-md-none"></div><form class="navbar-form flex-grow-1 order-7 order-md-3"><input id=userinput class="form-control is-search" type=search placeholder="Search docs..." aria-label="Search docs..." autocomplete=off><div id=suggestions class="shadow bg-white rounded"></div></form></div></div></header><div class="wrap container" role=document><div class=content><div class="row justify-content-left"><div class=col-12><article><div class=blog-header><h1>Roll your sleeves! Let us do some partial derivatives.</h1><p><small>Posted August 14, 2020 by <a class="stretched-link position-relative" href=https://www.gowrishankar.info/contributors/gowri-shankar/>Gowri Shankar</a>&nbsp;&dash;&nbsp;<strong>3&nbsp;min read</strong></small><p></div><p class=lead>In this post, we shall explore a shallow neural network with a single hidden layer and the math behind back propagation algorithm, gradient descent</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=kn>from</span> <span class=nn>IPython.display</span> <span class=kn>import</span> <span class=n>Image</span>
</code></pre></div><h2 id=forward-and-backpropagation-algorithm>Forward and Backpropagation Algorithm</h2><p>An associated source code will be published shortly.</p><p>In this post, we shall explore a shallow Dense Neural Network(DNN) with</p><ul><li>an Input Layer</li><li>a Single Hidden Layer and</li><li>an Output Layer</li></ul><p>The typical architecture of a feed forward network has the above units.
Where</p><ul><li>$x$ is a vector that represents the input values,</li><li>$\hat y$ is a vector that represents the predictions and</li><li>$h$ is vector that represents the hidden layer, $h$ is a hyperparameter selected based on the context of the problem</li></ul><p>Further to the above 3 parameters, connection weights $[W_1, W_2]$ and connection biases $[b_1, b_2]$ are the other matrices and vectors used in a DNN</p><h2 id=architecture-of-a-shallow-dnn>Architecture of a Shallow DNN</h2><p><strong>6 Inputs, 9 Hidden Nodes, 4 Outputs</strong></p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>display</span><span class=p>(</span><span class=n>Image</span><span class=p>(</span><span class=s2>&#34;/kaggle/input/sample-images/shallow-dnn.png&#34;</span><span class=p>))</span>
</code></pre></div><p><img src=output_3_0.png alt=png></p><p>Following are the processes we shall cover in the quest of understanding a simple Neural Network architecture,</p><ul><li>Forward Propagation</li><li>Cross Entropy Loss (covered in a separate post)</li><li>Activation Functions<ul><li>Softmax</li><li>ReLU</li></ul></li><li>Backpropagation</li><li>Gradient Descent</li><li>Extraction of features of the hidden layer</li></ul><p>From the above Diagram&mldr;</p><ul><li>There are 6 Inputs of dimension $(I \times 1)$ ie (6 X 1)</li><li>There are 9 nodes in the Hidden Layer of dimension $(N \times 1)$ ie (9 X 1)</li><li>There are 4 nodes in the Output Layer of dimension $(O \times 1)$ ie (4 X 1)</li></ul><h2 id=forward-propagation>Forward Propagation</h2><p>From the inputs and weights, biases of the hidden layer,
$$z_1 = W_1x + b_1 \tag{1}$$
$$h = ReLU(z_1) \tag{2}$$
From hidden layer ($h$) and weights, biases to the output layer
$$z_2 = W_2h + b_2 \tag{3}$$
$$\hat y = softmax(z_2) \tag{4}$$</p><h3 id=dimension-analysis>Dimension Analysis</h3><p>How to select the weights, let us do dimensionality reduction
Let us examine the dimensions of all elements in the equation 1
$$z_1 = W_1h + b_1$$
$$dimensions$$
$$N \times 1 = [?, ?] \times [I, 1] + [?, ?]$$
$$naturally$$
$$N \times 1 = [N, I] \times [I, 1] + [N, 1]$$
$$hence$$
$$9 \times 1 = [9, 6] \times [6, 1] + [9, 1]$$
Hence $N \times I $ is the dimension of the weight matrix $W_1$</p><p>Let us examine the dimensions of all elements in the equation 2
$$z_2 = W_2h + b_2$$
$$dimensions$$
$$O \times 1 = [?, ?] \times [N, 1] + [?, ?]$$
$$then$$
$$O \times 1 = [O, N] \times [N, 1] + [O, 1]$$
$$hence$$
$$4 \times 1 = [4, 9] \times [9, 1] + [4, 1]$$
Hence $O \times N $ is the dimension of the weight matrix $W_2$</p><h2 id=cross-entropy-loss>Cross Entropy Loss</h2><p>Our goal is to minimize the loss $J$
$$J = - \sum\limits_{k=1}^V y_k \log\hat y_k \tag{5}$$
<a href=https://www.linkedin.com/pulse/costloss-function-binary-classification-gowri-shankar/>Cost(Loss) Function for Binary Classification - A Deep Dive</a></p><h2 id=backpropagation---pull-your-sleeves-and-do-some-partial-derivatives>Backpropagation - Pull your sleeves and do some partial derivatives</h2><p>We calculated the $\hat y$ during forward propagation and now our goal is to optimize the weights by minimizing the loss. This is done by calculating the partial derivatives wrt $[W_2, b_2]$ and then $[W_1, b_1]$
$$\frac{\partial J}{\partial W_1} = ReLU\left( W^T_2(\hat y - y)\right)x^T \tag{6}$$
$$\frac{\partial J}{\partial W_2} = (\hat y - y)h^T \tag{7}$$
$$\frac{\partial J}{\partial b_1} = ReLU\left(W^T_2(\hat y - y)\right) \tag{8}$$
$$\frac{\partial J}{\partial b_2} = \hat y - y \tag{9}$$</p><h2 id=gradient-descent>Gradient Descent</h2><p>Gradient descent is the process during which the weights and biases are updated by subtracting $\alpha$ times the learning rate of calculated gradients from the original matrices and biases</p><p>$$W_1 := W_1 - \alpha\frac{\partial J}{\partial {W_{1}}} \tag{10}$$
$$W_2 := W_2 - \alpha\frac{\partial J}{\partial W_2} \tag{11}$$
$$b_1 := b_1 - \alpha\frac{\partial J}{\partial b_1} \tag{12}$$
$$b_2 := b_2 - \alpha\frac{\partial J}{\partial b_2} \tag{13}$$</p><p><a href=https://www.linkedin.com/pulse/demystifying-gradient-descent-classification-problem-gowri-shankar/>Demystifying Gradient Descent - A Deep Dive</a></p><div class=highlight><pre class=chroma><code class=language-python data-lang=python>
</code></pre></div></article></div><nav class="docs-toc d-none d-xl-block col-xl-4" aria-label="Secondary navigation"><div class=page-links><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#forward-and-backpropagation-algorithm>Forward and Backpropagation Algorithm</a></li><li><a href=#architecture-of-a-shallow-dnn>Architecture of a Shallow DNN</a></li><li><a href=#forward-propagation>Forward Propagation</a><ul><li><a href=#dimension-analysis>Dimension Analysis</a></li></ul></li><li><a href=#cross-entropy-loss>Cross Entropy Loss</a></li><li><a href=#backpropagation---pull-your-sleeves-and-do-some-partial-derivatives>Backpropagation - Pull your sleeves and do some partial derivatives</a></li><li><a href=#gradient-descent>Gradient Descent</a></li></ul></nav></div></nav></div></div></div><footer class="footer text-muted"><div class=container><div class=row><div class="col-lg-8 order-last order-lg-first"><ul class=list-inline><li class=list-inline-item>Copyright 2021 <a href=https://www.gowrishankar.info/>GowriShankar.info</a> All rights reserved</li></ul></div><div class="col-lg-8 order-first order-lg-last text-lg-right"><ul class=list-inline></ul></div></div></div></footer><script src=https://www.gowrishankar.info/main.f6b484f556ad1f3bcf6061082139a2f21fa759f13930c39a25fe4a9f78f35e64122c2d86dffd56e67b292dabbda4095d8077194f196e0e348441c106a9f3d40e.js integrity="sha512-9rSE9VatHzvPYGEIITmi8h+nWfE5MMOaJf5Kn3jzXmQSLC2G3/1W5nspLau9pAldgHcZTxluDjSEQcEGqfPUDg==" crossorigin=anonymous defer></script><script src=https://www.gowrishankar.info/index.min.7a602ace1becaf60dc69027361aa7b13a225462f6639889a37f19953d6b8927cd3940da9251c0d016bebf2fb65357d9a23de4dd04e817f627f2902a88348b45c.js integrity="sha512-emAqzhvsr2DcaQJzYap7E6IlRi9mOYiaN/GZU9a4knzTlA2pJRwNAWvr8vtlNX2aI95N0E6Bf2J/KQKog0i0XA==" crossorigin=anonymous defer></script></body></html>